{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543562b1-37c1-42c1-83ed-5400c7f7fe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "# %matplotlib widget\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "sns.set_palette(['#1E1E1E', '#BB3524', '#F5D54A', '#384827', '#282F44'])\n",
    "sns.set_context('paper')\n",
    "sns.set_style({'axes.axisbelow': True, \n",
    "               'axes.edgecolor': '.15',\n",
    "               'axes.facecolor': 'white',\n",
    "               'axes.grid': False, \n",
    "               'axes.labelcolor': '.15', \n",
    "               'figure.facecolor': 'white', \n",
    "               'grid.color': '.15',\n",
    "               'grid.linestyle': ':', \n",
    "               'grid.alpha': .5, \n",
    "               'image.cmap': 'Greys', \n",
    "               'legend.frameon': False, \n",
    "               'legend.numpoints': 1, \n",
    "               'legend.scatterpoints': 1,\n",
    "               'lines.solid_capstyle': 'butt', \n",
    "               'axes.spines.right': False, \n",
    "               'axes.spines.top': False,  \n",
    "               'text.color': '.15',  \n",
    "               'xtick.top': False, \n",
    "               'ytick.right': False, \n",
    "               'xtick.color': '.15',\n",
    "               'xtick.direction': 'out', \n",
    "               'ytick.color': '.15', \n",
    "               'ytick.direction': 'out', \n",
    "              })\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "FONT_SIZE_PT = 5\n",
    "matplotlib.rcParams['font.family'] = 'Arial'\n",
    "matplotlib.rcParams['font.size'] = FONT_SIZE_PT\n",
    "matplotlib.rcParams['axes.labelsize'] = FONT_SIZE_PT\n",
    "matplotlib.rcParams['axes.titlesize'] = FONT_SIZE_PT\n",
    "matplotlib.rcParams['figure.titlesize'] = FONT_SIZE_PT\n",
    "matplotlib.rcParams['xtick.labelsize'] = FONT_SIZE_PT\n",
    "matplotlib.rcParams['ytick.labelsize'] = FONT_SIZE_PT\n",
    "matplotlib.rcParams['legend.fontsize'] = FONT_SIZE_PT\n",
    "matplotlib.rcParams['legend.title_fontsize'] = FONT_SIZE_PT\n",
    "\n",
    "matplotlib.rcParams['xtick.major.size'] = matplotlib.rcParams['ytick.major.size'] = 2\n",
    "matplotlib.rcParams['xtick.major.width'] = matplotlib.rcParams['ytick.major.width'] = 0.5\n",
    "\n",
    "\n",
    "matplotlib.rcParams['xtick.minor.size'] = matplotlib.rcParams['ytick.minor.size'] = 1\n",
    "\n",
    "matplotlib.rcParams['xtick.minor.width'] = matplotlib.rcParams['ytick.minor.width'] = 0.5\n",
    "\n",
    "matplotlib.rcParams['axes.linewidth'] = 0.5\n",
    "matplotlib.rcParams['lines.linewidth'] = 0.5\n",
    "matplotlib.rcParams['grid.linewidth'] = 0.25\n",
    "matplotlib.rcParams['patch.linewidth'] = 0.25\n",
    "matplotlib.rcParams['lines.markeredgewidth'] = 0.25\n",
    "matplotlib.rcParams['lines.markersize'] = 2\n",
    "\n",
    "FIVE_MM_IN_INCH = 0.19685\n",
    "DPI = 600\n",
    "matplotlib.rcParams['figure.figsize'] = (10 * FIVE_MM_IN_INCH, 9 * FIVE_MM_IN_INCH)\n",
    "matplotlib.rcParams['savefig.dpi'] = DPI\n",
    "matplotlib.rcParams['figure.dpi'] = DPI // 4\n",
    "\n",
    "\n",
    "#http://phyletica.org/matplotlib-fonts/\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18f1ff-5c8b-493c-bb17-2a6535509605",
   "metadata": {},
   "outputs": [],
   "source": [
    "HAVE_SNAKEMAKE = 'snakemake' in locals()\n",
    "\n",
    "if HAVE_SNAKEMAKE:\n",
    "    # Code assumes a list..\n",
    "    input_consolidated_stats = [snakemake.input.consolidated_tsv]\n",
    "    \n",
    "    param_correlation_method = snakemake.params['correlation_method']\n",
    "    param_input_header_separator = snakemake.params.get('input_header_sep', '__')\n",
    "    threads = snakemake.threads\n",
    "    \n",
    "    output_stats_mi_tsv = snakemake.output.stats_mi_tsv\n",
    "    output_stats_corr_tsv = snakemake.output.stats_corr_tsv\n",
    "    \n",
    "    output_heatmap_mi_pdf = snakemake.output.heatmap_mi_pdf\n",
    "    output_heatmap_corr_pdf = snakemake.output.heatmap_corr_pdf\n",
    "    output_diagnostic_plots = snakemake.output.diagnostic_plots\n",
    "    \n",
    "    param_hue_palette_factor_type = snakemake.params.get('hue_palette_factor_type', None)\n",
    "    param_shape_palette_factor_type = snakemake.params.get('shape_palette_factor_type', None)\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"No snakemake -- DEBUG MODE\")\n",
    "    \n",
    "    _OUTDIR = '.nb-testing-outputs'\n",
    "    if not os.path.isdir(_OUTDIR):\n",
    "        os.makedirs(_OUTDIR)\n",
    "    \n",
    "    _bin_size = 1000\n",
    "    _pseudocount = 100\n",
    "    _min_periods = 1\n",
    "    \n",
    "    input_consolidated_stats = []\n",
    "    for _cell_line in ['K562']:\n",
    "        input_consolidated_stats.append(f'../../output/final/analysis/params_{_bin_size}bp_pc_{_pseudocount}_mp_{_min_periods}/{_cell_line}/consolidated_tables/bedstats_consolidated_{_cell_line}_{_bin_size}bp_params_pc_{_pseudocount}_mp_{_min_periods}_from_bed.csv.gz')\n",
    "       \n",
    "       \n",
    "    param_correlation_method = 'kendall'\n",
    "    param_input_header_separator = '__'\n",
    "    \n",
    "    output_heatmap_mi_pdf = os.path.join(_OUTDIR, 'heatmap.mi.pdf')\n",
    "    output_heatmap_corr_pdf = os.path.join(_OUTDIR, 'heatmap.corr.pdf')\n",
    "    \n",
    "    output_stats_mi_tsv = os.path.join(_OUTDIR, 'stats.mi.tsv')\n",
    "    output_stats_corr_tsv = os.path.join(_OUTDIR, 'stats.corr.tsv')\n",
    "    \n",
    "    output_diagnostic_plots = os.path.join(_OUTDIR, 'diagnostic')\n",
    "    \n",
    "    param_hue_palette_factor_type = None\n",
    "    param_shape_palette_factor_type = None\n",
    "    \n",
    "    threads = 8\n",
    "    \n",
    "param_fdr_method = 'fdr_bh'\n",
    "param_alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502b4bd6-e6e4-4b79-9ea9-919f4ba7b14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dd3612-1330-496d-9988-cdfb08963c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445567dc-bba1-4127-b99e-ecb84851922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if we need to\n",
    "if not os.path.isdir(output_diagnostic_plots):\n",
    "    os.makedirs(output_diagnostic_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03416c7-be74-41f7-af4c-c11a0578800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if param_hue_palette_factor_type is None:\n",
    "    param_hue_palette_factor_type = helpers.get_default_hue_palette_factor_type()\n",
    "    \n",
    "if param_shape_palette_factor_type is None:\n",
    "    param_shape_palette_factor_type = helpers.get_default_shape_palette_factor_type()\n",
    "    \n",
    "print(f'{param_hue_palette_factor_type=}')\n",
    "print(f'{param_shape_palette_factor_type}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92c89c8-661f-450f-9d7a-d7223933765a",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ee305-cdea-42af-9307-dbaf3e6faaef",
   "metadata": {},
   "source": [
    "So the previous step of the pipeline was aimed to assemble the data into an easily-digestable format.\n",
    "\n",
    "The goal of this notebook is to provide some sort of summary statistic regarding the relationships between MARCS data and the ChIP-seq data.\n",
    "Particularly, we will be interested to see whether on average, a particular histone/chromatin feature ChIP-seq explains more or less entropy of proteins associated with MARCS feature compared to a random protein.\n",
    "\n",
    "This is what the data looks like as we import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9185113-c634-4081-bf80-b830da89833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for filename in input_consolidated_stats:\n",
    "    basename = os.path.basename(filename)\n",
    "    \n",
    "    df = pd.read_csv(filename, index_col=0)\n",
    "    df.columns = pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            c.split(param_input_header_separator) for c in df.columns\n",
    "        ],\n",
    "        names=['header', 'column']\n",
    "    )\n",
    "    \n",
    "    data[basename] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668a26f4-adae-4547-8ceb-acba3ef36ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c148695-bfb1-431a-9cb5-f4722353356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(data.values())).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b02150-9fd1-4b4b-a938-93cb717fb3af",
   "metadata": {},
   "source": [
    "As the MARCS annotations are aggregated by `marcs_gene_label` column, \n",
    "and the latter is aggregated by `factor`, in order to avoid some of the double counting, \n",
    "we will reaggregate the dataframe by Factor column, taking the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d66f80-caa6-40a4-808e-0718a779c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reaggregated = {}\n",
    "\n",
    "for k, df in data.items():\n",
    "    # see helpers.py\n",
    "    data_reaggregated[k] = helpers.reaggregate_by_factor(df, factor_col=('metadata', 'factor'))\n",
    "    data_reaggregated[k]['marcs_feature_significant_category'] = data_reaggregated[k]['marcs_feature_significant_category'].fillna('No data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f96e86b-d94c-4a8d-b860-a498adbc35af",
   "metadata": {},
   "source": [
    "The aggregatted data looks like this (see the index by `factor`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6a7a0-21d7-4fdb-98f8-5769e3d09548",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(data_reaggregated.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f403c1-f3d2-4227-beb3-9306bbedd98c",
   "metadata": {},
   "source": [
    "We will now try to summarise this data.\n",
    "\n",
    "Primarily we will be doing a Mann-Whitney-U statistical test on the scores.\n",
    "We will be comparing a MARCS category, e.g. \"Recruited by H3K4me3\", to proteins in \"other\" category, specifically proteins neither recruited, nor excluded by H3K4me3, and proteins that we have no estimate for.\n",
    "For visualisation purposes, we will additionally compute the mean log2 difference between the scores for proteins in MARCS category vs scores in the control category.\n",
    "\n",
    "\n",
    "This is implemented in the get stats function in `helpers.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb9277-8ee6-49f4-9c35-6e19f17712ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.get_stats??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75deee11-ae39-43d4-b18c-93ad2e1bc604",
   "metadata": {},
   "source": [
    "The `normalised_mi` stats can be computed in a fairly easy way using this `get_stats` function.\n",
    "We do not need to worry about the normalised_mi being equal to zero due to smoothing upstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197559e8-949b-4034-9371-69e900101f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "recruited_group = 'Strongly recruited'\n",
    "excluded_group = 'Strongly excluded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc606b-3ac8-4e64-8a0e-eceb59aaac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['normalised_mi'].min().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e53a7-3333-45d3-accc-7bd65d79497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in data_reaggregated.values():\n",
    "    # Make sure there are no zero MIs\n",
    "    assert df['normalised_mi'].min().min() > 0\n",
    "\n",
    "stats_mi = {\n",
    "    k: helpers.get_stats(\n",
    "        df, \n",
    "        column='normalised_mi', \n",
    "        control_groups=['Neither', 'No data'], \n",
    "        test_groups=[recruited_group, excluded_group],\n",
    "        fdr_method=None # don't correct at this point\n",
    "    )\n",
    "    for k, df in data_reaggregated.items()\n",
    "}\n",
    "\n",
    "# concatenate the results from multiple reaggregated datasets\n",
    "stats_mi = pd.concat(stats_mi.values(), keys=stats_mi.keys())\n",
    "stats_mi.index.names = ['dataset'] + list(stats_mi.index.names[1:])\n",
    "\n",
    "# Have easier indices if we only have one dataset...\n",
    "if stats_mi.index.get_level_values('dataset').nunique() == 1:\n",
    "    stats_mi = stats_mi.droplevel('dataset')\n",
    "\n",
    "# Jointly adjust p-vals where they are not null\n",
    "# Correct here\n",
    "stats_mi = helpers.adjust_pvals(stats_mi, fdr_method=param_fdr_method, fdr_alpha=param_alpha).sort_values(by='p-val corrected')\n",
    "\n",
    "stats_mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f8ad84-0ef0-4757-8767-072c36a8b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_mi.to_csv(output_stats_mi_tsv, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0370eea2-1f70-4ad4-b3f1-f392ba862311",
   "metadata": {},
   "source": [
    "Correlation can be exactly equal to zero. In such cases to avoid division by zero we need to replace these values with a small other value.\n",
    "In this particualr case we will use the smallest absolute non-zero correlation from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f6e673-14c5-49de-b08f-e9c13e9bbb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_corr = {}\n",
    "\n",
    "for k, df in data_reaggregated.items():\n",
    "    min_nonzero_abs_corr = df[f'{param_correlation_method}_correlation'].stack().abs()\n",
    "    min_nonzero_abs_corr = min_nonzero_abs_corr[min_nonzero_abs_corr > 0].min()\n",
    "    \n",
    "    print(f'{k=}: {min_nonzero_abs_corr=}')\n",
    "        \n",
    "    stats_corr[k] = helpers.get_stats(\n",
    "        df, \n",
    "        column=f'{param_correlation_method}_correlation', \n",
    "        control_groups=['Neither', 'No data'], \n",
    "        test_groups=[recruited_group, excluded_group], \n",
    "        log2=False,\n",
    "        fdr_method=None, # Don't correct here\n",
    "    )\n",
    "\n",
    "    \n",
    "# concatenate the results from multiple reaggregated datasets\n",
    "stats_corr = pd.concat(stats_corr.values(), keys=stats_corr.keys())\n",
    "stats_corr.index.names = ['dataset'] + list(stats_corr.index.names[1:])\n",
    "\n",
    "# Have easier indices if we only have one dataset...\n",
    "if stats_corr.index.get_level_values('dataset').nunique() == 1:\n",
    "    stats_corr = stats_corr.droplevel('dataset')\n",
    "    \n",
    "# Jointly adjust p-vals where they are not null\n",
    "stats_corr = helpers.adjust_pvals(stats_corr, fdr_method=param_fdr_method, fdr_alpha=param_alpha).sort_values(by='p-val corrected')\n",
    "\n",
    "stats_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54eac38-9be6-4ba1-afb4-6280697824c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_corr.to_csv(output_stats_corr_tsv, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827a0cc9-06fc-49e3-9ea6-eb724ed12730",
   "metadata": {},
   "source": [
    "Now that we have the stats dataframe we can plot the results.\n",
    "We will use heatmap to visualise the matrices of the results.\n",
    "\n",
    "Let's prepare these matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6192b8-adef-401b-bd87-251322c8ada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = {}\n",
    "matrix_masks = {}\n",
    "matrix_pvals = {}\n",
    "\n",
    "main_matrix = 'mi'\n",
    "\n",
    "for name, stat_df, diff_col in [('mi', stats_mi, 'mean_log2_diff'), ('corr', stats_corr, 'mean_diff')]:\n",
    "    _df = stat_df.copy()\n",
    "    \n",
    "    # We can make the matrix wide by unstacking marcs_feature and group\n",
    "    matrices[name] = _df[diff_col].unstack(['marcs_feature', 'group'])\n",
    "    \n",
    "    matrix_masks[name] = ~(_df['significant'].unstack(['marcs_feature', 'group']).fillna(False))\n",
    "    matrix_pvals[name] = _df['p-val corrected'].unstack(['marcs_feature', 'group'])\n",
    "\n",
    "# Equalise indices\n",
    "for name in matrices.keys():\n",
    "    \n",
    "    for dict_ in [matrices, matrix_masks, matrix_pvals]:\n",
    "        dict_[name] =  dict_[name].reindex(matrices[main_matrix].index, axis=0).reindex(matrices[main_matrix].columns, axis=1)\n",
    "    \n",
    "    # Mask=True implies \"not significant\", therefore fill should be with True\n",
    "    matrix_masks[name] = matrix_masks[name].fillna(True)\n",
    "    \n",
    "# Let's drop columns that are always null (in all matrices)\n",
    "always_null = pd.Series(True, index=matrices['mi'].columns)\n",
    "for m in matrices.values():\n",
    "    always_null &= m.isnull().all()\n",
    "\n",
    "to_drop = always_null[always_null].index\n",
    "print(f\"Dropping these columns as they are always null:\\n{to_drop}\")\n",
    "for name in matrices.keys():\n",
    "    matrices[name] = matrices[name].loc(axis=1)[~always_null]\n",
    "    matrix_masks[name] = matrix_masks[name].loc(axis=1)[~always_null]\n",
    "    matrix_pvals[name] = matrix_pvals[name].loc(axis=1)[~always_null]\n",
    "    \n",
    "    \n",
    "# Let's also drop the null rows in MI matrix\n",
    "\n",
    "always_null_row = matrices['mi'].isnull().all(axis=1)\n",
    "for name in matrices.keys():\n",
    "    matrices[name] = matrices[name].loc[~always_null_row]\n",
    "    matrix_masks[name] = matrix_masks[name].loc[~always_null_row]\n",
    "    matrix_pvals[name] = matrix_pvals[name].loc[~always_null_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b8caa4-e96e-4728-bbf3-1e267d8d4f41",
   "metadata": {},
   "source": [
    "Now that we have the matrices, let's compute the linkage for their rows and columns.\n",
    "\n",
    "\n",
    "We will be clustering only the `main_matrix`. The earlier version was concatenating the matrices but that is counterproductive\n",
    "\n",
    "We will be using correlation distance for both rows and columns\n",
    "Complete linkage for both with optimal leaf ordering.\n",
    "\n",
    "NaNs will be filled with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d3479b-72ba-4b6b-ace9-ab363f64d993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy as hcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c61ae1-f238-495f-bdfb-6fd1b5cc1bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_concatenated_matrix = matrices[main_matrix] # pd.concat(matrices.values(), keys=matrices.keys(), axis=1)\n",
    "col_concatenated_matrix = matrices[main_matrix] # pd.concat(matrices.values(), keys=matrices.keys(), axis=0)\n",
    "\n",
    "linkage_rows = hcluster.linkage(row_concatenated_matrix.fillna(0), metric='correlation', method='complete', optimal_ordering=True)\n",
    "linkage_rows_order = row_concatenated_matrix.index[hcluster.dendrogram(linkage_rows, no_plot=True)['leaves']]\n",
    "\n",
    "linkage_cols = hcluster.linkage(col_concatenated_matrix.T.fillna(0), metric='correlation', method='complete', optimal_ordering=True)\n",
    "linkage_cols_order = col_concatenated_matrix.columns[hcluster.dendrogram(linkage_cols, no_plot=True)['leaves']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22aa52-237e-43d8-8d57-2a03e9c7d7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "280e0cbd-2cdd-4a95-ad4b-5183eb4989e9",
   "metadata": {},
   "source": [
    "# Seaborn Heatmaps\n",
    "\n",
    "Now that we have the matrices we can simply plot them.\n",
    "First, plot the (rather ugly) seaborn heatmaps to make sure we're not plotting the data wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba153d3-5af1-4a0a-b4d6-6c16c78b6917",
   "metadata": {},
   "source": [
    "MI Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec263a9-ada2-4b19-8edd-e4296c43822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_name = 'mi'\n",
    "_matrix = matrices[_name]\n",
    "_mask = matrix_masks[_name]\n",
    "\n",
    "# Helps with interpretation of labels\n",
    "if isinstance(_matrix.index, pd.MultiIndex):\n",
    "    yticklabels = ['-'.join([x[1], x[0]]) for x in _matrix.index]\n",
    "else:\n",
    "    yticklabels = _matrix.index\n",
    "    \n",
    "assert _matrix.columns.equals(_mask.columns)\n",
    "assert _matrix.index.equals(_mask.index)\n",
    "\n",
    "_annot = _mask.applymap(lambda x: '*' if not x else '')\n",
    "\n",
    "_row_linkage = linkage_rows\n",
    "_col_linkage = linkage_cols\n",
    "                  \n",
    "_cmap = sns.clustermap(\n",
    "    _matrix.fillna(0), #mask=_mask, \n",
    "    annot=_annot,\n",
    "   cmap='RdBu_r',\n",
    "   row_linkage=_row_linkage,\n",
    "   col_linkage=_col_linkage,\n",
    "   figsize=(FIVE_MM_IN_INCH*30, FIVE_MM_IN_INCH*27),\n",
    "   yticklabels=yticklabels,\n",
    "   xticklabels=1,\n",
    "   center=0,\n",
    "   fmt='',\n",
    "   linewidth=0.1, linecolor='black',\n",
    "   robust=True,\n",
    ")\n",
    "\n",
    "_cmap.ax_heatmap.xaxis.set_tick_params(length=0)\n",
    "_cmap.ax_heatmap.yaxis.set_tick_params(length=0)\n",
    "_cmap.cax.set_ylabel(f\"Average difference in\\n {_name}, log2\")\n",
    "_cmap.ax_col_dendrogram.set_title(f\"{_name}\")\n",
    "\n",
    "# _cmap.savefig(output_heatmap_mi_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570bf616-5a44-4c1e-a9fd-82a6142f23cc",
   "metadata": {},
   "source": [
    "Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed011f56-a4d2-4a4c-9c9d-1a96ffa546a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_name = 'corr'\n",
    "_matrix = matrices[_name]\n",
    "_mask = matrix_masks[_name]\n",
    "\n",
    "# Helps with interpretation of labels\n",
    "if isinstance(_matrix.index, pd.MultiIndex):\n",
    "    yticklabels = ['-'.join([x[1], x[0]]) for x in _matrix.index]\n",
    "else:\n",
    "    yticklabels = _matrix.index\n",
    "    \n",
    "\n",
    "assert _matrix.columns.equals(_mask.columns)\n",
    "assert _matrix.index.equals(_mask.index)\n",
    "\n",
    "_annot = _mask.applymap(lambda x: '*' if not x else '')\n",
    "\n",
    "_row_linkage = linkage_rows\n",
    "_col_linkage = linkage_cols\n",
    "                  \n",
    "_cmap = sns.clustermap(\n",
    "    _matrix.fillna(0), #mask=_mask, \n",
    "    annot=_annot,\n",
    "   cmap='RdBu_r',\n",
    "   row_linkage=_row_linkage,\n",
    "   col_linkage=_col_linkage,\n",
    "   figsize=(FIVE_MM_IN_INCH*30, FIVE_MM_IN_INCH*30),\n",
    "   yticklabels=yticklabels,\n",
    "   xticklabels=1,\n",
    "   center=0,\n",
    "   fmt='',\n",
    "   linewidth=0.1, linecolor='black',\n",
    "   robust=True,\n",
    ")\n",
    "\n",
    "_cmap.ax_heatmap.xaxis.set_tick_params(length=0)\n",
    "_cmap.ax_heatmap.yaxis.set_tick_params(length=0)\n",
    "_cmap.cax.set_ylabel(f\"Average difference in\\n {_name}, log2\")\n",
    "_cmap.ax_col_dendrogram.set_title(f\"{_name}\")\n",
    "\n",
    "# _cmap.savefig(output_heatmap_corr_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a5379f-8bad-4959-865e-cf7b557a666f",
   "metadata": {},
   "source": [
    "# Styled heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6114a769-0a3a-45ea-95e9-9a7a16b8d70f",
   "metadata": {},
   "source": [
    "Now let's redo the heatmaps as styled scatterplots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da4c545-29fc-4964-b04a-ee38b535fecb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "_name = 'mi'\n",
    "_title = 'normalised MI'\n",
    "_matrix = matrices[_name].copy()\n",
    "_mask = matrix_masks[_name].copy()\n",
    "_pvals = matrix_pvals[_name].copy()\n",
    "\n",
    "_row_order = linkage_rows_order.copy()\n",
    "_col_order = linkage_cols_order.copy()\n",
    "\n",
    "_row_linkage = linkage_rows\n",
    "_col_linkage = linkage_cols\n",
    "\n",
    "_row_coords = pd.Series(\n",
    "    np.arange(len(_row_order), 0, -1) * 10.0 - 5, # the weird multiplication are due to matplotlib's dendrogram madness\n",
    "    # Reversed order matches the dendrogram better\n",
    "    index=list(reversed(_row_order)),\n",
    "    name='row_coord'\n",
    ")\n",
    "\n",
    "_col_coords = pd.Series(\n",
    "    np.arange(1, len(_col_order)+1) * 10.0 - 5, # the weird multiplications are due to matplotlib's dendrogram madness\n",
    "    index=_col_order,\n",
    "    name='col_coord'\n",
    ")\n",
    "\n",
    "# Don't mess with the wide format, convert all into narrow\n",
    "# To do this make sure we have normal indices column names\n",
    "for _df in [_matrix, _mask, _pvals, _row_coords, _col_coords]:\n",
    "    if isinstance(_df.index, pd.MultiIndex):\n",
    "        _df.index = ['-'.join(ix) for ix in _df.index]\n",
    "    \n",
    "    if isinstance(_df, pd.DataFrame) and isinstance(_df.columns, pd.MultiIndex):\n",
    "        _df.columns = ['-'.join(ix) for ix in _df.columns]\n",
    "\n",
    "# A couple operations to force the matrix into long format:\n",
    "_matrix_long = pd.concat([_matrix, _mask, _pvals], keys=['matrix', 'mask', 'pvals'])\n",
    "_matrix_long = _matrix_long.stack()\n",
    "_matrix_long.index.names = ['measurement', 'row_name', 'col_name']\n",
    "_matrix_long = _matrix_long.unstack('measurement')\n",
    "_matrix_long = _matrix_long.join(_row_coords, on='row_name')\n",
    "_matrix_long = _matrix_long.join(_col_coords, on='col_name')\n",
    "\n",
    "# Bin the p-values\n",
    "# If you change the bins, change the legend code below too\n",
    "_matrix_long['pvals_binned'] = pd.cut(_matrix_long['pvals'], bins=[0, 0.01, 0.05, 1.0])\n",
    "assert len(_matrix_long.pvals_binned.cat.categories) == 3\n",
    "_pval_kwarg_dict = dict(zip(_matrix_long.pvals_binned.cat.categories, [dict(s=30, marker='s', edgecolor='#525252', linewidth=0.45), dict(s=20, marker='s', edgecolor='#525252', linewidth=0.45), dict(s=10, marker='s', edgecolor='#CDCDCD', linewidth=0.45)]))\n",
    "_ns_pval_group = _matrix_long.pvals_binned.cat.categories[-1]\n",
    "\n",
    "# Force the colour scale for min and max\n",
    "_vmin = -2.1 # don't make this a round number (will help with the ticks)\n",
    "_vmax = 2.1\n",
    "\n",
    "\n",
    "_cmap = 'RdBu_r'\n",
    "\n",
    "n_rows, n_cols = _matrix.shape\n",
    "\n",
    "fig = plt.figure(\n",
    "    figsize=(\n",
    "        # four mm * (columns + dendrogram) + [labels]\n",
    "        4/5*FIVE_MM_IN_INCH*(n_cols+5) + FIVE_MM_IN_INCH*5, \n",
    "        # four mm * (rows + dendrogram)\n",
    "        4/5*FIVE_MM_IN_INCH*(n_rows+5+5),\n",
    "    ),\n",
    "    constrained_layout=True,\n",
    ")\n",
    "\n",
    "gs = GridSpec(3, 2, width_ratios=[5, n_cols], height_ratios=[5, n_rows, 5], wspace=0.05, hspace=0.05)\n",
    "\n",
    "ax_heatmap = fig.add_subplot(gs[1,1])\n",
    "\n",
    "ax_heatmap.xaxis.tick_top()\n",
    "ax_heatmap.yaxis.tick_right()\n",
    "\n",
    "sns.despine(ax=ax_heatmap, bottom=False, right=False, top=False, left=False)\n",
    "ax_heatmap.xaxis.set_tick_params(length=0)\n",
    "ax_heatmap.yaxis.set_tick_params(length=0)\n",
    "\n",
    "# Draw heatmap\n",
    "for _pval_group, _submatrix in _matrix_long.groupby('pvals_binned'):\n",
    "    \n",
    "     _colours = ax_heatmap.scatter(\n",
    "        _submatrix['col_coord'], \n",
    "        _submatrix['row_coord'], \n",
    "        c=_submatrix['matrix'], \n",
    "        vmin=_vmin, vmax=_vmax, \n",
    "        cmap=_cmap,\n",
    "        **_pval_kwarg_dict[_pval_group],\n",
    "    )\n",
    "\n",
    "\n",
    "# Draw dendrograms\n",
    "ax_col_dendrogram = fig.add_subplot(gs[2,1], sharex=ax_heatmap)\n",
    "ax_row_dendrogram = fig.add_subplot(gs[1,0], sharey=ax_heatmap)\n",
    "\n",
    "for ax, link, orient in [\n",
    "    (ax_col_dendrogram, _col_linkage, 'bottom'),\n",
    "    (ax_row_dendrogram, _row_linkage, 'left')\n",
    "]:\n",
    "    \n",
    "    hcluster.dendrogram(\n",
    "        link, \n",
    "        ax=ax, \n",
    "        orientation=orient, \n",
    "        link_color_func= lambda x: 'black'\n",
    "    )\n",
    "    \n",
    "    sns.despine(ax=ax, left=True, bottom=True, right=True, top=True)\n",
    "    \n",
    "    ax.xaxis.set_tick_params(length=0)\n",
    "    ax.yaxis.set_tick_params(length=0)\n",
    "    \n",
    "    for tick in ax.get_yticklabels():\n",
    "        tick.set_visible(False)\n",
    "    \n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_visible(False)\n",
    "    \n",
    "    \n",
    "# Set ticks\n",
    "_yticks = _row_coords.sort_values()\n",
    "ax_heatmap.set_yticks(_yticks.values)\n",
    "ax_heatmap.set_yticklabels(_yticks.index)\n",
    "\n",
    "_xticks = _col_coords.sort_values()\n",
    "ax_heatmap.set_xticks(_xticks.values)\n",
    "ax_heatmap.set_xticklabels(_xticks.index, rotation=90)\n",
    "\n",
    "# Inverting yaxis matches seaborn \n",
    "ax_heatmap.invert_yaxis() \n",
    "\n",
    "# Add legends\n",
    "ax_legend = fig.add_subplot(gs[0,0])\n",
    "\n",
    "# Colourbar\n",
    "cax = ax_legend.inset_axes([0.0, 0.7, 1.0, 0.3])\n",
    "fig.colorbar(_colours, ax=ax_legend, cax=cax, orientation='horizontal')\n",
    "cax.set_title(f\"Difference in {_title}, log2\")\n",
    "cax.minorticks_on()\n",
    "\n",
    "\n",
    "# P-val legend\n",
    "ax_legend.text(0.5, 0.4, 'MWU p-val (B/H):', ha='center', va='center')\n",
    "assert len(_pval_kwarg_dict) == 3\n",
    "ax_legend.scatter(0.25, 0.25, color='#bdbdbd', **_pval_kwarg_dict[_matrix_long.pvals_binned.cat.categories[0]])\n",
    "ax_legend.scatter(0.5, 0.25, color='#bdbdbd', **_pval_kwarg_dict[_matrix_long.pvals_binned.cat.categories[1]])\n",
    "ax_legend.scatter(0.75, 0.25, color='#bdbdbd', **_pval_kwarg_dict[_matrix_long.pvals_binned.cat.categories[2]])\n",
    "\n",
    "ax_legend.text(0.25, 0.15, '<0.01', ha='center', va='top')\n",
    "ax_legend.text(0.5, 0.15, '<0.05', ha='center', va='top')\n",
    "ax_legend.text(0.75, 0.15, 'n.s', ha='center', va='top')\n",
    "\n",
    "ax_legend.set_ylim([0, 1])\n",
    "ax_legend.set_xlim([0, 1])\n",
    "ax_legend.axis('off')\n",
    "\n",
    "\n",
    "plt.savefig(output_heatmap_mi_pdf, bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c7a37a-684f-4310-b413-5639d83e4806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "_name = 'corr'\n",
    "_title = f'{param_correlation_method}\\ncorrelation'\n",
    "_matrix = matrices[_name].copy()\n",
    "_mask = matrix_masks[_name].copy()\n",
    "_pvals = matrix_pvals[_name].copy()\n",
    "\n",
    "_row_order = linkage_rows_order.copy()\n",
    "_col_order = linkage_cols_order.copy()\n",
    "\n",
    "_row_linkage = linkage_rows\n",
    "_col_linkage = linkage_cols\n",
    "\n",
    "_row_coords = pd.Series(\n",
    "    np.arange(len(_row_order), 0, -1) * 10.0 - 5, # the weird multiplication are due to matplotlib's dendrogram madness\n",
    "    # Reversed order matches the dendrogram better\n",
    "    index=list(reversed(_row_order)),\n",
    "    name='row_coord'\n",
    ")\n",
    "\n",
    "_col_coords = pd.Series(\n",
    "    np.arange(1, len(_col_order)+1) * 10.0 - 5,  # the weird multiplication are due to matplotlib's dendrogram madness\n",
    "    index=_col_order,\n",
    "    name='col_coord'\n",
    ")\n",
    "\n",
    "# Don't mess with the wide format, convert all into narrow\n",
    "# To do this make sure we have normal indices column names\n",
    "for _df in [_matrix, _mask, _pvals, _row_coords, _col_coords]:\n",
    "    if isinstance(_df.index, pd.MultiIndex):\n",
    "        _df.index = ['-'.join(ix) for ix in _df.index]\n",
    "    \n",
    "    if isinstance(_df, pd.DataFrame) and isinstance(_df.columns, pd.MultiIndex):\n",
    "        _df.columns = ['-'.join(ix) for ix in _df.columns]\n",
    "\n",
    "# A couple operations to force the matrix into long format:\n",
    "_matrix_long = pd.concat([_matrix, _mask, _pvals], keys=['matrix', 'mask', 'pvals'])\n",
    "_matrix_long = _matrix_long.stack()\n",
    "_matrix_long.index.names = ['measurement', 'row_name', 'col_name']\n",
    "_matrix_long = _matrix_long.unstack('measurement')\n",
    "_matrix_long = _matrix_long.join(_row_coords, on='row_name')\n",
    "_matrix_long = _matrix_long.join(_col_coords, on='col_name')\n",
    "\n",
    "# Bin the p-values\n",
    "# If you change the bins, change the legend code below too\n",
    "_matrix_long['pvals_binned'] = pd.cut(_matrix_long['pvals'], bins=[0, 0.01, 0.05, 1.0])\n",
    "assert len(_matrix_long.pvals_binned.cat.categories) == 3\n",
    "_pval_kwarg_dict = dict(zip(_matrix_long.pvals_binned.cat.categories, [dict(s=30, marker='s', edgecolor='#525252', linewidth=0.45), dict(s=20, marker='s', edgecolor='#525252', linewidth=0.45), dict(s=10, marker='s', edgecolor='#CDCDCD', linewidth=0.45)]))\n",
    "_ns_pval_group = _matrix_long.pvals_binned.cat.categories[-1]\n",
    "\n",
    "# Force the colour scale for min and max\n",
    "_vmin = -0.25 # don't make this a round number (will help with the ticks)\n",
    "_vmax = 0.25\n",
    "_cmap = 'RdBu_r'\n",
    "\n",
    "n_rows, n_cols = _matrix.shape\n",
    "\n",
    "fig = plt.figure(\n",
    "    figsize=(\n",
    "        # four mm * (columns + dendrogram) + [labels]\n",
    "        4/5*FIVE_MM_IN_INCH*(n_cols+5) + FIVE_MM_IN_INCH*5, \n",
    "        # four mm * (rows + dendrogram)\n",
    "        4/5*FIVE_MM_IN_INCH*(n_rows+5+5),\n",
    "    ),\n",
    "    constrained_layout=True,\n",
    ")\n",
    "\n",
    "gs = GridSpec(3, 2, width_ratios=[5, n_cols], height_ratios=[5, n_rows, 5], wspace=0.05, hspace=0.05)\n",
    "\n",
    "ax_heatmap = fig.add_subplot(gs[1,1])\n",
    "\n",
    "ax_heatmap.xaxis.tick_top()\n",
    "ax_heatmap.yaxis.tick_right()\n",
    "\n",
    "sns.despine(ax=ax_heatmap, bottom=False, right=False, top=False, left=False)\n",
    "ax_heatmap.xaxis.set_tick_params(length=0)\n",
    "ax_heatmap.yaxis.set_tick_params(length=0)\n",
    "\n",
    "# Draw heatmap\n",
    "for _pval_group, _submatrix in _matrix_long.groupby('pvals_binned'):\n",
    "    \n",
    "     _colours = ax_heatmap.scatter(\n",
    "        _submatrix['col_coord'], \n",
    "        _submatrix['row_coord'], \n",
    "        c=_submatrix['matrix'], \n",
    "        vmin=_vmin, vmax=_vmax, \n",
    "        cmap=_cmap,\n",
    "        **_pval_kwarg_dict[_pval_group],\n",
    "    )\n",
    "\n",
    "\n",
    "# Draw dendrograms\n",
    "ax_col_dendrogram = fig.add_subplot(gs[2,1], sharex=ax_heatmap)\n",
    "ax_row_dendrogram = fig.add_subplot(gs[1,0], sharey=ax_heatmap)\n",
    "\n",
    "for ax, link, orient in [\n",
    "    (ax_col_dendrogram, _col_linkage, 'bottom'),\n",
    "    (ax_row_dendrogram, _row_linkage, 'left')\n",
    "]:\n",
    "    \n",
    "    hcluster.dendrogram(\n",
    "        link, \n",
    "        ax=ax, \n",
    "        orientation=orient, \n",
    "        link_color_func= lambda x: 'black'\n",
    "    )\n",
    "    \n",
    "    sns.despine(ax=ax, left=True, bottom=True, right=True, top=True)\n",
    "    \n",
    "    ax.xaxis.set_tick_params(length=0)\n",
    "    ax.yaxis.set_tick_params(length=0)\n",
    "    \n",
    "    for tick in ax.get_yticklabels():\n",
    "        tick.set_visible(False)\n",
    "    \n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_visible(False)\n",
    "    \n",
    "    \n",
    "# Set ticks\n",
    "_yticks = _row_coords.sort_values()\n",
    "ax_heatmap.set_yticks(_yticks.values)\n",
    "ax_heatmap.set_yticklabels(_yticks.index)\n",
    "\n",
    "_xticks = _col_coords.sort_values()\n",
    "ax_heatmap.set_xticks(_xticks.values)\n",
    "ax_heatmap.set_xticklabels(_xticks.index, rotation=90)\n",
    "\n",
    "# Add legends\n",
    "ax_legend = fig.add_subplot(gs[0,0])\n",
    "\n",
    "# Colourbar\n",
    "cax = ax_legend.inset_axes([0.0, 0.7, 1.0, 0.3])\n",
    "fig.colorbar(_colours, ax=ax_legend, cax=cax, orientation='horizontal')\n",
    "cax.set_title(f\"Mean difference in {_title}\")\n",
    "cax.minorticks_on()\n",
    "\n",
    "ax_heatmap.invert_yaxis()\n",
    "\n",
    "# P-val legend\n",
    "ax_legend.text(0.5, 0.4, 'MWU p-val (B/H):', ha='center', va='center')\n",
    "assert len(_pval_kwarg_dict) == 3\n",
    "ax_legend.scatter(0.25, 0.25, color='#bdbdbd', **_pval_kwarg_dict[_matrix_long.pvals_binned.cat.categories[0]])\n",
    "ax_legend.scatter(0.5, 0.25, color='#bdbdbd', **_pval_kwarg_dict[_matrix_long.pvals_binned.cat.categories[1]])\n",
    "ax_legend.scatter(0.75, 0.25, color='#bdbdbd', **_pval_kwarg_dict[_matrix_long.pvals_binned.cat.categories[2]])\n",
    "\n",
    "ax_legend.text(0.25, 0.15, '<0.01', ha='center', va='top')\n",
    "ax_legend.text(0.5, 0.15, '<0.05', ha='center', va='top')\n",
    "ax_legend.text(0.75, 0.15, 'n.s', ha='center', va='top')\n",
    "\n",
    "ax_legend.set_ylim([0, 1])\n",
    "ax_legend.set_xlim([0, 1])\n",
    "ax_legend.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig(output_heatmap_corr_pdf, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7491a-d9b4-41ca-9e49-208c4ac06679",
   "metadata": {},
   "source": [
    "Additionally, we should make some make diagnostic scatterplots, one for each chip factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b337374-03a9-4394-b435-a405f670064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "marcs_feature_order = helpers.MARCS_FEATURE_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be82f6c-ad20-4135-8d8d-077e0bcab1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(data_reaggregated.values()))['marcs_feature_significant_category'].stack().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39018053-b91f-4f40-bece-8edcb57144cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hue_palette = {\n",
    "#     'Neither': '#1E1E1E',\n",
    "#     'No data': '#1E1E1E',\n",
    "#     'Recruited': '#b2182b',\n",
    "#     'Excluded': '#2166ac'\n",
    "# }\n",
    "\n",
    "shape_map = {\n",
    "    'Neither': dict(edgecolor='#CDCDCD', linewidth=0.35, zorder=0, s=4, alpha=.8),\n",
    "    'No data': dict(zorder=0, s=5, marker='x', alpha=.8),\n",
    "    recruited_group: dict(edgecolor='#525252', linewidth=.35, zorder=1, s=8, alpha=.8),\n",
    "    excluded_group: dict(edgecolor='#525252', linewidth=.35, zorder=1, s=8, alpha=.8),\n",
    "}\n",
    "\n",
    "FEATURES_PALETTE = {\n",
    "    'Neither': '#bdbdbd',\n",
    "    'No data': '#bdbdbd',\n",
    "    recruited_group: '#BA5047',\n",
    "    excluded_group: '#4B82B6',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e8487a-7939-4eb3-875c-46aebbc86f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "from numpy.random import RandomState\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "# from contextlib import suppress\n",
    "\n",
    "import gc\n",
    "\n",
    "def make_diagnostic_plots(data, *, filename, chip_factor, palette_features, shape_map_features, \n",
    "                          marcs_feature_order=marcs_feature_order, approx_number_of_labels=20, logx = False, no_y=False):\n",
    "    \n",
    "    x_col = ('normalised_mi', chip_factor)\n",
    "    y_col = (f'{param_correlation_method}_correlation', chip_factor)\n",
    "\n",
    "    ylabel = \"{} Correlation\".format(param_correlation_method.capitalize())\n",
    "\n",
    "    if not logx:\n",
    "         xlabel = \"Normalised MI\"\n",
    "    else:\n",
    "        data = data.copy()\n",
    "        data['normalised_mi'] = data['normalised_mi'].apply(np.log2)\n",
    "        xlabel = \"Normalised MI (log2)\"\n",
    "    \n",
    "    lines = ['y=0']\n",
    "    \n",
    "    yticks = True\n",
    "\n",
    "    df = data.reset_index().copy()\n",
    "    \n",
    "    # If there is no y axis (no correlation), \n",
    "    # Plot ranks of x column instead\n",
    "    if no_y or y_col not in df.columns:\n",
    "        df[('rank_x', 'rank_x')] = df[x_col].rank(ascending=True, pct=True)\n",
    "\n",
    "        y_col = ('rank_x', 'rank_x')\n",
    "        ylabel = 'Rank'\n",
    "        lines = None\n",
    "        yticks = False\n",
    "\n",
    "        no_y = True\n",
    "    \n",
    "    with PdfPages(filename) as pdf:\n",
    "        \n",
    "#    with suppress():\n",
    "        \n",
    "        for marcs_feature in [None] + list(marcs_feature_order):\n",
    "            \n",
    "            fig = plt.figure(figsize=(10*FIVE_MM_IN_INCH, 10*FIVE_MM_IN_INCH))\n",
    "            ax_main = plt.gca()\n",
    "            \n",
    "            if marcs_feature is None:\n",
    "           \n",
    "                helpers.make_plot(\n",
    "                    df=df,\n",
    "                    x_col=x_col,\n",
    "                    xlabel=xlabel,\n",
    "                    y_col=y_col,\n",
    "                    ylabel=ylabel,\n",
    "                    annot_col=('metadata', 'factor'),\n",
    "                    hue_col=('metadata', 'factor_type'),\n",
    "                    shape_col=('metadata', 'factor_type'),\n",
    "                    axes=ax_main,\n",
    "                    legend=False,\n",
    "                    title=chip_factor,\n",
    "                    shape_map=param_shape_palette_factor_type,\n",
    "                    hue_palette=param_hue_palette_factor_type,\n",
    "                    approx_number_of_labels=approx_number_of_labels,\n",
    "                    lines=lines,\n",
    "                    do_not_change_limits=True,\n",
    "                    adjust_text_kws=dict(lim=250),\n",
    "                )            \n",
    "            else:\n",
    "                \n",
    "                # Hack: Propagate factor types into feature significant category so non-proteins apear with their symbols and not \"x\"\n",
    "                _df = df.copy()\n",
    "                new_column = ('marcs_feature_significant_category_or_feature_type', marcs_feature)\n",
    "                _df[new_column] = _df[('marcs_feature_significant_category', marcs_feature)].copy()\n",
    "                \n",
    "                not_protein = _df[('metadata', 'factor_type')] != 'protein'\n",
    "                _df.loc[not_protein, new_column] = _df.loc[not_protein, ('metadata', 'factor_type')]\n",
    "                \n",
    "                _hue_col = _shape_col = new_column\n",
    "\n",
    "                _ycol = y_col\n",
    "\n",
    "                # I we don't have y, then make a \"jitter\" axis across the shape_map_features \n",
    "                _hue_palette_order = list(shape_map_features.keys())\n",
    "                if no_y:\n",
    "                    _df =  _df.loc[_df[('metadata', 'factor_type')] == 'protein'].copy()\n",
    "                    _ycol = ('jitter_y', 'jitter_y')\n",
    "                    _df[_ycol] = _df[_shape_col].apply(_hue_palette_order.index)\n",
    "\n",
    "                    random = RandomState(42)\n",
    "                    _df[_ycol] += random.uniform(-0.25, 0.25, len(_df))\n",
    "                \n",
    "\n",
    "                helpers.make_plot(\n",
    "                    df=_df,\n",
    "                    x_col=x_col,\n",
    "                    xlabel=xlabel,\n",
    "                    y_col=_ycol,\n",
    "                    ylabel=None if no_y else ylabel,\n",
    "                    annot_col=('metadata', 'factor'),\n",
    "                    approx_number_of_labels=approx_number_of_labels,\n",
    "                    # This is hacky....\n",
    "                    label_strategy=set(_df[_df[_shape_col].isin([recruited_group, excluded_group])].index),\n",
    "                    hue_col=_hue_col,\n",
    "                    axes=ax_main,\n",
    "                    legend=False,\n",
    "                    title=f'{chip_factor} vs MARCS {marcs_feature}',\n",
    "                    hue_palette={**param_hue_palette_factor_type, **palette_features},\n",
    "                    shape_col=_shape_col,\n",
    "                    shape_map={**param_shape_palette_factor_type, **shape_map_features},\n",
    "                    lines=lines,\n",
    "                    do_not_change_limits=True,\n",
    "                    adjust_text_kws=dict(lim=100),\n",
    "                )\n",
    "                \n",
    "            \n",
    "            if not yticks:\n",
    "                ax_main.set_yticks([])\n",
    "        \n",
    "            sns.despine(ax=ax_main, offset=3)\n",
    "            \n",
    "            pdf.savefig(bbox_inches='tight')\n",
    "            # There's some sort of memory leak going on, let's try to avoid that\n",
    "            plt.close(fig)\n",
    "            plt.close('all')\n",
    "            plt.close()\n",
    "            gc.collect()\n",
    "        \n",
    "        \n",
    "    # There's some sort of memory leak going on, let's try to avoid that\n",
    "    plt.close('all')\n",
    "    plt.close()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bccea3-b918-46ee-8cbc-b018cf3dc070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for dataset, dataframe in tqdm(data_reaggregated.items(), total=len(data_reaggregated)):\n",
    "    diagnostic_plot_columns = df['normalised_mi'].columns\n",
    "    \n",
    "    for col in tqdm(diagnostic_plot_columns):\n",
    "        \n",
    "        for logx in [False, True]:\n",
    "            for no_y in [False, True]:\n",
    "                if col.startswith('state') and not no_y:\n",
    "                    continue\n",
    "                    \n",
    "                fname = re.sub('[^a-z0-9A-Z\\-\\_]+', '-', col)\n",
    "                prefix = (re.sub('[^a-z0-9A-Z\\-\\_]+', '-', dataset) + '-') if len(data_reaggregated) > 1 else ''\n",
    "\n",
    "                suffix = 'log2-scale' if logx else 'natural-scale'\n",
    "                if no_y:\n",
    "                    suffix += '-no_y'\n",
    "                    \n",
    "                full_filename = os.path.join(output_diagnostic_plots, f'diagnostic-{prefix}{fname}-{suffix}.pdf')\n",
    "                                \n",
    "                make_diagnostic_plots(\n",
    "                    dataframe, chip_factor=col,\n",
    "                    approx_number_of_labels=10 if no_y else 30,\n",
    "                    palette_features=FEATURES_PALETTE,\n",
    "                    shape_map_features=shape_map,\n",
    "                    logx=logx,\n",
    "                    no_y=no_y,\n",
    "                    filename=full_filename,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492c176d-7263-419f-91a8-12c1e2453019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
