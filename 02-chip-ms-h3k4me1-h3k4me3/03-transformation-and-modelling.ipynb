{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227bf404-c7d9-47a9-87a6-a54a207e7511",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "sns.set_palette('Dark2')\n",
    "sns.set_context('paper')\n",
    "sns.set_style({'axes.axisbelow': True, \n",
    "               'axes.edgecolor': '.15',\n",
    "               'axes.facecolor': 'white',\n",
    "               'axes.grid': True, \n",
    "               'axes.labelcolor': '.15', \n",
    "               'figure.facecolor': 'white', \n",
    "               'grid.color': '.15',\n",
    "               'grid.linestyle': ':', \n",
    "               'grid.alpha': .5, \n",
    "               'image.cmap': 'Greys', \n",
    "               'legend.frameon': False, \n",
    "               'legend.numpoints': 1, \n",
    "               'legend.scatterpoints': 1,\n",
    "               'lines.solid_capstyle': 'butt', \n",
    "               'axes.spines.right': False, \n",
    "               'axes.spines.top': False,  \n",
    "               'text.color': '.15',  \n",
    "               'xtick.top': False, \n",
    "               'ytick.right': False, \n",
    "               'xtick.color': '.15',\n",
    "               'xtick.direction': 'out', \n",
    "               'ytick.color': '.15', \n",
    "               'ytick.direction': 'out', \n",
    "              })\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "FONT_SIZE_PT = 5\n",
    "matplotlib.rcParams['font.family'] = 'Arial'\n",
    "matplotlib.rcParams['font.size'] = FONT_SIZE_PT\n",
    "matplotlib.rcParams['axes.labelsize'] = FONT_SIZE_PT\n",
    "matplotlib.rcParams['axes.titlesize'] = FONT_SIZE_PT\n",
    "matplotlib.rcParams['figure.titlesize'] = FONT_SIZE_PT\n",
    "matplotlib.rcParams['xtick.labelsize'] = FONT_SIZE_PT\n",
    "matplotlib.rcParams['ytick.labelsize'] = FONT_SIZE_PT\n",
    "matplotlib.rcParams['legend.fontsize'] = FONT_SIZE_PT\n",
    "matplotlib.rcParams['legend.title_fontsize'] = FONT_SIZE_PT\n",
    "\n",
    "matplotlib.rcParams['xtick.major.size'] = matplotlib.rcParams['ytick.major.size'] = 2\n",
    "matplotlib.rcParams['xtick.major.width'] = matplotlib.rcParams['ytick.major.width'] = 0.5\n",
    "\n",
    "\n",
    "matplotlib.rcParams['xtick.minor.size'] = matplotlib.rcParams['ytick.minor.size'] = 1\n",
    "\n",
    "matplotlib.rcParams['xtick.minor.width'] = matplotlib.rcParams['ytick.minor.width'] = 0.5\n",
    "\n",
    "matplotlib.rcParams['axes.linewidth'] = 0.5\n",
    "matplotlib.rcParams['lines.linewidth'] = 0.5\n",
    "matplotlib.rcParams['grid.linewidth'] = 0.25\n",
    "matplotlib.rcParams['patch.linewidth'] = 0.25\n",
    "matplotlib.rcParams['lines.markeredgewidth'] = 0.25\n",
    "matplotlib.rcParams['lines.markersize'] = 2\n",
    "\n",
    "FIVE_MM_IN_INCH = 0.19685\n",
    "DPI = 600\n",
    "matplotlib.rcParams['figure.figsize'] = (10 * FIVE_MM_IN_INCH, 9 * FIVE_MM_IN_INCH)\n",
    "matplotlib.rcParams['savefig.dpi'] = DPI\n",
    "matplotlib.rcParams['figure.dpi'] = DPI // 4\n",
    "\n",
    "\n",
    "#http://phyletica.org/matplotlib-fonts/\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d964a0-afb5-4369-b5d9-50fb0479e40c",
   "metadata": {},
   "source": [
    "# (03) Transformation and modeling\n",
    "\n",
    "This notebook handles the core of the analysis:\n",
    "\n",
    "1. Transformation of the numeric data into format that can be read by the model\n",
    "2. Normalisation of the data\n",
    "3. Modelling of differential enrichments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8857c038-af64-4340-8cb9-baa53d2ee0ef",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973b915f-4f20-4b4c-b1c5-075e3ac18eb7",
   "metadata": {},
   "source": [
    "Input file (the numeric data from previous notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb9c1d-5bad-425a-9820-a268ef792a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "INPUT_NUMERIC_DATA = pathlib.Path('outputs') / '01-extracting' / 'data_numeric.csv'\n",
    "assert INPUT_NUMERIC_DATA.is_file()\n",
    "\n",
    "INPUT_METADATA = pathlib.Path('outputs') / '01-extracting' / 'data_metadata.csv'\n",
    "assert INPUT_METADATA.is_file()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac8e46-da21-48a3-b224-7071c7034aba",
   "metadata": {},
   "source": [
    "Output directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7773b5d6-cb45-4f7e-b758-4231532624ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "OUTPUT_DIRECTORY = pathlib.Path('outputs') / '03-transformation-and-modelling'\n",
    "\n",
    "if not OUTPUT_DIRECTORY.is_dir():\n",
    "    OUTPUT_DIRECTORY.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19293f1-b1cb-4e3d-a86b-1ca28f22a317",
   "metadata": {},
   "source": [
    "Parameters, constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef12656e-8cd1-47d0-9093-042f6a958db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do analysis, we will require that:\n",
    "#- at least this many (below) values per condition are set (for the condition analysed)\n",
    "MINIMUM_NUMBER_OF_OBSERVED_VALUES_PER_CONDITION = 2\n",
    "\n",
    "# Cutoff for BH-adjusted p-values\n",
    "FDR_THRESHOLD = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4136f04a-2252-4f0d-af8b-28d59e961488",
   "metadata": {},
   "source": [
    "## Reading Metadata\n",
    "\n",
    "Reading the metadata from previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9781413a-6531-4fd1-8a5e-9d1b59774fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_metadata = pd.read_csv(\n",
    "    INPUT_METADATA,\n",
    "    index_col=0,\n",
    ")\n",
    "data_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96dc2ba-9808-4557-9ded-75e6c8c8f164",
   "metadata": {},
   "source": [
    "## Reading numeric data\n",
    "\n",
    "Read the numeric data from previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0dedbd-151d-4e37-a08e-c62c09767c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric = pd.read_csv(\n",
    "    INPUT_NUMERIC_DATA,\n",
    "    index_col=0\n",
    ")\n",
    "data_numeric.columns.name = 'Experiment_Replicate'\n",
    "data_numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39020ea-60f4-47ba-a3e6-ff30e22b074b",
   "metadata": {},
   "source": [
    "For this notebook it will also help to establish a set of \"Headers\" which describe what each column means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f05d8ba-eaf2-4b1c-8e47-1908c6723bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = []\n",
    "for col in data_numeric.columns:\n",
    "    headers.append([col] + list(col.split('_')))\n",
    "    \n",
    "headers = pd.DataFrame(headers, columns=[data_numeric.columns.name, 'Experiment', 'Replicate']).set_index('Experiment_Replicate')\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a444c1c8-0956-4b45-b1d9-c1b7205795e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff8e6b3-522b-493e-b1b4-0d031f86b869",
   "metadata": {},
   "source": [
    "The data we are loading here is MS intensities; they generally make more sense in log scale. \n",
    "We will therefore transform the data to `log2` here and work with that from now on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3620892-0325-4013-801e-086be928f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric_log2 = data_numeric.apply(np.log2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74fe2de-b971-47c7-9076-7256196d2e90",
   "metadata": {},
   "source": [
    "## EDA (log2 transformed, unnormalised, unfiltered)\n",
    "\n",
    "### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a3aec9-62ae-42e1-9da8-b7d110cb2f77",
   "metadata": {},
   "source": [
    "The first thing that is very obvious is that this data has many missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ec8f4-988a-41ea-8fc4-3150e1219cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msgno\n",
    "msgno.matrix(data_numeric_log2, figsize=(20*FIVE_MM_IN_INCH, 20*FIVE_MM_IN_INCH), fontsize=6)\n",
    "\n",
    "plt.title(\"Distribution of missing values in log2-transformed, unnormalised, and unfiltered data\")\n",
    "plt.xlabel(\"Data columns (experiments)\")\n",
    "plt.ylabel(\"Data rows (proteins)\")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / '01-EDA-log2-transformed-unnormalised-unfiltered-missing-values.png'\n",
    "_caption = \"\"\"\n",
    "Distribution of missing values in the log2-transformed, unnormalised, and unfiltered data. The heatmap above displays all 1046 proteins in the data in rows, and all experiments in columns. Dark values indicate the presence of a value, while white space indicates the absence of one. The sparkline on the right hand side counts the number of values per protein, which ranges from zero to 12 in this dataset.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48596bae-9b71-469d-bd6a-373be3f4ee04",
   "metadata": {},
   "source": [
    "### Normality\n",
    "\n",
    "The analyses below will assume that the log2-transformed data is approximately normal. \n",
    "\n",
    "We can qualititatively verify this by plotting the data distributions.\n",
    "\n",
    "We see that the fit is not perfect but is perhaps acceptable. \n",
    "We will therefore not worry about the normality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ada6d-f97a-445f-a38d-eaba38daeefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "def _plot_norm_fit(values, *args, color=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Fits a normal distribution to the data and plots its PDF\n",
    "    \"\"\"\n",
    "    mu, std = norm.fit(values)\n",
    "\n",
    "    min_ = values.min()\n",
    "    max_ = values.max()\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    xs = np.linspace(min_, max_, 100)\n",
    "    ys = norm.pdf(xs, mu, std)\n",
    "    \n",
    "    ax.plot(xs, ys, **kwargs, color='black', label='Normal fit')\n",
    "    ax.axvline(mu, color='black')\n",
    "    \n",
    "    ax.text(\n",
    "        0.98, 0.98, '\\n'.join([r'$\\mu = {:.2f}$'.format(mu),r'$\\sigma = {:.2f}$'.format(std)]),\n",
    "        ha='right', va='top', transform=ax.transAxes\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "# Convert data to long format\n",
    "_df = data_numeric_log2.copy()\n",
    "_df = _df.stack(_df.columns.names)\n",
    "_df.name = 'value'\n",
    "_df = _df.reset_index()\n",
    "\n",
    "# Plot the histograms and normal distribution fits\n",
    "fgrid = sns.FacetGrid(col='Experiment_Replicate', col_wrap=3, col_order=data_numeric_log2.columns, data=_df, size=FIVE_MM_IN_INCH*5)\n",
    "fgrid.map(sns.histplot, 'value', stat='density')\n",
    "fgrid.map(_plot_norm_fit, 'value')\n",
    "fgrid.set_titles('{col_name}')\n",
    "\n",
    "plt.suptitle(\"Normal fit of log2-transformed, unnormalised, and unfiltered data\", y=1.05)\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / '01-EDA-log2-transformed-unnormalised-unfiltered-normal-fit.pdf'\n",
    "_caption = \"\"\"\n",
    "Distribution of non-null values of log2-transformed, unnormalised, and unfiltered dataset. \n",
    "The rows facet the four experiment types, while the columns split the data per replicate.\n",
    "The green distribution plot shows the histogram of observed log2 values in the data.\n",
    "The black curve shows the corresponding normal distribution fit to the data, whose parameters are written in the top-right corner.\n",
    "The black vertical line indicates the mean ($\\mu$) estimate.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aae394-dfcc-43d1-acad-8edd25862e6a",
   "metadata": {},
   "source": [
    "### PCA \n",
    "\n",
    "And finally the PCA of unnormalissed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6480c60f-c23f-41f4-95d0-0a67d23d5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adjustText import adjust_text\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Note that we only look at the rows that are all not null:\n",
    "projected = pca.fit_transform(data_numeric_log2.dropna(axis=0).T)\n",
    "\n",
    "projected = pd.DataFrame(projected, index=data_numeric_log2.columns, columns=['PC1', 'PC2'])\n",
    "explained_variance = pd.Series(pca.explained_variance_ratio_, index=projected.columns)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Experiment', data=projected.join(headers))\n",
    "\n",
    "texts = []\n",
    "for ix, row in projected.iterrows():\n",
    "    texts.append(ax.text(row['PC1'], row['PC2'], ix, ha='center', va='center'))\n",
    "\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Increasing ylim helps place the H3K4me3 labels\n",
    "ax.set_ylim(ylim[0]*1.1, ylim[1] * 1.1)\n",
    "\n",
    "adjust_text(texts, projected['PC1'].values, projected['PC2'].values, arrowprops=dict(arrowstyle='-'))\n",
    "\n",
    "ax.set_xlabel(\"PC1 ({:.2%} variance)\".format(explained_variance.loc['PC1']))\n",
    "ax.set_ylabel(\"PC2 ({:.2%} variance)\".format(explained_variance.loc['PC2']))\n",
    "ax.grid(False)\n",
    "ax.set_title(\"PCA, log2-transformed data, unfiltered,\\nunnormalised, only proteins with no missing values\")\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Experiment')\n",
    "\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / '01-EDA-log2-transformed-unnormalised-unfiltered-PCA.pdf'\n",
    "_caption = \"\"\"\n",
    "Principal Component embedding of log2-transformed, unnormalised, and unfiltered dataset, including only the rows which have no missing values.\n",
    "The principal component 1 (PC1) is plotted on the X axis, while the PC2 is plotted on the Y. \n",
    "The explained variance ratio is highlighted in parentheses.\n",
    "\n",
    "Note that the experiment types (colour, see legend on the right) do not cluster together as this data is unnormalised.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4d6fea-8b25-4521-ba64-ac9570f70951",
   "metadata": {},
   "source": [
    "## Normalisation\n",
    "\n",
    "For normalisation of these intensities, Andrey suggests the use of Histones H4 and H2B, namely:\n",
    "\n",
    "- `Histone H4 OS=Homo sapiens OX=9606 GN=H4C1 PE=1 SV=2`\n",
    "- `Histone H2B type 1-C/E/F/G/I OS=Homo sapiens OX=9606 GN=H2BC4 PE=1 SV=4`\n",
    "\n",
    "In our transformed data these correspond to indices `H4C1` and `H2BC4` (from the `GN=` annotation in the description)\n",
    "\n",
    "In subsequent analyses I have found that using more than these two proteins performs a bit better.\n",
    "Here's the list of all proteins with the wort `histone` in description, that we observe in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f105eb7-2937-4c1e-854e-b51c0984b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_metadata[data_metadata['Description'].str.lower().str.contains('histone')].sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc70ffa1-dac8-4f0b-bcd1-579c45e424de",
   "metadata": {},
   "source": [
    "Out of which the list `HISTONES` contains the set of core histone proteins and their variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564952ea-28dd-496b-bc33-28170880c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTONES = [\n",
    "    'H2AC20', \n",
    "    'H2AC21', \n",
    "    'H2AW', \n",
    "    'H2AZ2', \n",
    "    'H2BC4',\n",
    "    'H2BU1', \n",
    "    'H3-2', \n",
    "    'H4C1',\n",
    "    'MACROH2A1', \n",
    "    'MACROH2A2'\n",
    "]\n",
    "\n",
    "# # Andrey suggests using these:\n",
    "# NORMALISATION_PROTEINS = ['H4C1', 'H2BC4']\n",
    "\n",
    "# I think perhaps it might be better to use the full collection of histone\n",
    "# proteins instead:\n",
    "\n",
    "NORMALISATION_PROTEINS = HISTONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e84dfa-894b-4c27-a316-f0b3b8a4f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric_log2_histones = data_numeric_log2.loc[HISTONES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b80bf32-8f4d-4bbb-b074-dead4d79ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric_log2_histones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb897ff5-64e9-47b8-824a-079ae38d1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "_cmap = sns.clustermap(\n",
    "    data_numeric_log2_histones,\n",
    "    cmap='viridis', robust=True, \n",
    "    figsize=(20*FIVE_MM_IN_INCH, 10*FIVE_MM_IN_INCH),\n",
    "    metric='correlation',\n",
    "    method='complete',\n",
    ")\n",
    "_cmap.ax_col_dendrogram.set_title(\"Unnormalised data (log2) for histone proteins only\")\n",
    "\n",
    "for tick in _cmap.ax_heatmap.get_yticklabels():\n",
    "    tick_text = tick.get_text()\n",
    "    if tick_text in NORMALISATION_PROTEINS:\n",
    "        tick.set_color('red')\n",
    "        \n",
    "_fname = OUTPUT_DIRECTORY / '02-normalisation-histones-heatmap-unnormalised.pdf'\n",
    "_caption = \"\"\"\n",
    "Boxplot showing the unnormalised log2 data for histone proteins only.\n",
    "The proteins are plotted on the Y axis, and the experiments on the X.\n",
    "\n",
    "Proteins selected as normalisation factors are highlighted in red.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2815d4f1-06c9-437c-b358-5a72371cfee8",
   "metadata": {},
   "source": [
    "Mark `NORMALISATION_PROTEINS` as \"Used for normalisation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a76ae42-4c2d-4f3f-9027-b183c7dd530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_norm = pd.Series(None, dtype=object, index=data_numeric_log2.index)\n",
    "comments_norm.loc[NORMALISATION_PROTEINS] = 'Used for normalisation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4054c30-01db-488d-8701-669230703460",
   "metadata": {},
   "source": [
    "Andrey used the sum of the (natural scale) intensities of these proteins as a normalisation factor;\n",
    "we tried working on a similar note and use the mean of the log2-transformed intensities for such histone proteins,\n",
    "however taking a median of M-offsets below seems to be a more robust estimator, the M-offsets can obtained by subtracting the row-wise mean from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180b21a3-6662-40ea-8852-7322bc67e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric_log2.sub(data_numeric_log2.mean(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dfde32-2119-44e3-b9c0-1803096e9406",
   "metadata": {},
   "source": [
    "### Normalisation factors calculation\n",
    "\n",
    "We will now compute the MA statistics from the unnormalised data.\n",
    "\n",
    "MA statistics are simply row-wise averages of the data (`avg` column, the `A` part)\n",
    "And the log2 offsets from this average (`log2_diff` column, the `M` part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc527620-0f9d-40e3-9f8c-763f42871451",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric_log2_row_averages = data_numeric_log2.mean(axis=1)\n",
    "data_numeric_log2_row_averages.name = 'avg'\n",
    "data_numeric_log2_row_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfa8c64-5c08-458f-b773-45188b4be3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric_log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a0bcf-27d4-4c2f-bb7b-2ffaa65babf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric_log2_diffs = data_numeric_log2.sub(data_numeric_log2_row_averages, axis=0)\n",
    "data_numeric_log2_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67691225-04b1-4536-a8d7-00c052d71e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric_log2_ma = data_numeric_log2_diffs.stack()\n",
    "data_numeric_log2_ma.name = 'log2_diff'\n",
    "data_numeric_log2_ma = data_numeric_log2_ma.reset_index().join(data_numeric_log2_row_averages, on='Label')\n",
    "data_numeric_log2_ma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22118ad8-bccb-42ca-b287-1d0dd948c8e1",
   "metadata": {},
   "source": [
    "We will now compute the normalisation factors from the `NORMALISATION_PROTEINS`.\n",
    "\n",
    "Andrey uses the sum of natural scale intensities, \n",
    "I have tried using the median of intensities (see below)\n",
    "\n",
    "But I now think that it is more appropriate to take the median of the M offsets of the `NORMALISATION_PROTEINS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e0dcbc-a499-4305-8de5-063de7cb3ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andrey's normfactors\n",
    "# normalisation_factors = (data_numeric_log2.loc[NORMALISATION_PROTEINS].apply(lambda x: np.power(2,x)).sum()).apply(np.log2)\n",
    "\n",
    "# My normfactors (attempt):\n",
    "# normalisation_factors = data_numeric_log2.loc[NORMALISATION_PROTEINS].median()\n",
    "\n",
    "# Get the normfactors from MA offsets\n",
    "_ma_data_for_normalisation_proteins = data_numeric_log2_ma[\n",
    "    data_numeric_log2_ma['Label'].isin(NORMALISATION_PROTEINS)\n",
    "]\n",
    "print(_ma_data_for_normalisation_proteins.shape)\n",
    "assert set(_ma_data_for_normalisation_proteins['Label'].unique()) == set(NORMALISATION_PROTEINS)\n",
    "\n",
    "\n",
    "normalisation_factors = _ma_data_for_normalisation_proteins.groupby('Experiment_Replicate')['log2_diff'].median()\n",
    "\n",
    "normalisation_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b6b137-7893-4468-abb4-df011a547ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ma_data_for_normalisation_proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f6ded9-7da3-4e94-b1eb-061dd2770e90",
   "metadata": {},
   "source": [
    "### Plots (Before)\n",
    "\n",
    "#### MA\n",
    "\n",
    "To illustrate the normalisation behaviour it is helpful to make some MA plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc39ba3-2e31-43a8-a3eb-4072a7e8cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adjustText import adjust_text\n",
    "def annotate_histones(x, y, *, color=None, data=None):\n",
    "    data_histones = data[data['Label'].isin(HISTONES)]\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    print(data_histones.shape)\n",
    "    ax.scatter(data_histones[x], data_histones[y], color=color)\n",
    "    \n",
    "    texts = []\n",
    "    for __, row in data_histones.iterrows():\n",
    "        texts.append(ax.text(row[x], row[y], row['Label'], color=color))\n",
    "        \n",
    "    adjust_text(texts, arrowprops=dict(arrowstyle='-'), ax=ax)\n",
    "    \n",
    "def add_horizontal_line(*args, color=None, data=None):\n",
    "    ax = plt.gca()\n",
    "    ax.axhline(0, linestyle='-',color='black', zorder=1)\n",
    "\n",
    "fgrid = sns.FacetGrid(col='Experiment_Replicate', col_wrap=3,\n",
    "                      size=FIVE_MM_IN_INCH*10,\n",
    "                      col_order=data_numeric_log2.columns, data=data_numeric_log2_ma)\n",
    "fgrid.map(sns.scatterplot, 'avg', 'log2_diff', alpha=.4, color='black')\n",
    "fgrid.map_dataframe(annotate_histones, 'avg', 'log2_diff')\n",
    "fgrid.map_dataframe(add_horizontal_line)\n",
    "fgrid.set_titles('{col_name}')\n",
    "\n",
    "for experiment, ax in fgrid.axes_dict.items():\n",
    "    ax.axhline(normalisation_factors.loc[experiment], color='r', linestyle='-')\n",
    "    ax.grid(False)\n",
    "\n",
    "plt.suptitle(\"MA plots of of log2-transformed, unnormalised, and unfiltered data\", y=1.05)\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / '02-normalisation-log2-transformed-unnormalised-unfiltered-ma-plot.pdf'\n",
    "_caption = \"\"\"\n",
    "MA-like plot for log2-transformed, unnormalised and unfiltered data.\n",
    "\n",
    "X axis plots mean(log2_intensity) across data rows (proteins). X axis is the same in every plot.\n",
    "Y axis plots log2_intensity - mean(log2_intensity) offsets for individual samples\n",
    "\n",
    "Histone proteins are highlighted in green.\n",
    "Red line indicates the normalisation factor estimate.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107cb2ad-4dc0-43c9-8306-191082050bf2",
   "metadata": {},
   "source": [
    "#### Boxplot\n",
    "\n",
    "Alternatively, we can plot all of this in a boxplot format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b77a06-a342-49f5-9c63-1b07dc28468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = data_numeric_log2.stack(list(data_numeric_log2.columns.names)).copy()\n",
    "_df.name = 'log2_intensity_unnormalised'\n",
    "_df = _df.reset_index()\n",
    "\n",
    "_df['is_histone'] = False\n",
    "_df.loc[_df['Label'].isin(HISTONES), 'is_histone'] = True\n",
    "\n",
    "_df_cov = pd.DataFrame(\n",
    "    # The + below recentres the normalisation factors around the mean of normalisation proteins\n",
    "    {'normalisation_factors': normalisation_factors + data_numeric_log2.loc[NORMALISATION_PROTEINS].mean(axis=1).mean()}, \n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(10*FIVE_MM_IN_INCH, 15*FIVE_MM_IN_INCH))\n",
    "ax = plt.gca()\n",
    "\n",
    "sns.pointplot(\n",
    "    y='Experiment_Replicate', x='normalisation_factors', \n",
    "    data=_df_cov.reset_index(),\n",
    "    order=data_numeric_log2.columns,\n",
    "    marker='.',\n",
    "    color='black',\n",
    "    label=\"Estimated covariates\",\n",
    ")\n",
    "\n",
    "sns.boxplot(\n",
    "    y='Experiment_Replicate', x='log2_intensity_unnormalised', \n",
    "    data=_df, hue='is_histone', showfliers=False,\n",
    "    order=data_numeric_log2.columns,\n",
    ")\n",
    "\n",
    "\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), title=\"Histone protein\")\n",
    "ax.set_xlabel(\"Unnormalised log2 intensity\")\n",
    "ax.set_ylabel(\"Experiment, Replicate\")\n",
    "ax.set_title(\"Distribution of intensities, log2-transformed,\\nunnormalised, unfiltered data\")\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / '02-normalisation-boxplot.pdf'\n",
    "_caption = \"\"\"\n",
    "Boxplot showing the distribution of signal intensities for non-histone proteins (green), and the histone proteins (orange), prior to normalisation.\n",
    "The intensity is plotted on the X axis, while the samples are facetted on the Y. The outliers (\"fliers\") are hidden.\n",
    "\n",
    "Black line shows the estimated normalisation offsets centred around the mean of normalisation proteins.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05e457d-ed9d-44ce-ba48-bb3108669c71",
   "metadata": {},
   "source": [
    "### The bit where normalisation is performed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a3b61-ef79-450f-a994-8ece4b91ce9b",
   "metadata": {},
   "source": [
    "To perform the normalisation we simply subtract the normalisation factors from the dataset.\n",
    "\n",
    "Note that as we are using (zero-centred) offsets for the normalisation, the intuitive meaning of the normalised intensities stays more or less the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a1dc2-0fe6-4366-b6a1-5a8410ce32eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric_log2_normalised = data_numeric_log2 - normalisation_factors\n",
    "# In the row below we need to make sure column order stays the same\n",
    "data_numeric_log2_normalised = data_numeric_log2_normalised[data_numeric_log2.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475913ea-a5c9-4a5b-9fbd-11584d6602da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal\n",
    "\n",
    "# The minus operator above works columnwise, we can confirm this\n",
    "for col in data_numeric_log2_normalised:\n",
    "    assert_array_equal(data_numeric_log2_normalised[col], data_numeric_log2[col] - normalisation_factors.loc[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a764775d-1fd1-4872-bdee-b40c159d901a",
   "metadata": {},
   "source": [
    "The normalisation has the effect of making the normalisation protein M-offsets constant\n",
    "(though not necessarily zero, as median is used, not mean):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b63705-d978-491a-8866-68e55c7fc208",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric_log2_normalised.sub(data_numeric_log2_normalised.mean(axis=1), axis=0).loc[NORMALISATION_PROTEINS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721d18ea-5c63-45eb-b759-8073a5b5dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric_log2_normalised.sub(data_numeric_log2_normalised.mean(axis=1), axis=0).loc[NORMALISATION_PROTEINS].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b2e7e-ac29-4002-9cf4-8291c0e2f758",
   "metadata": {},
   "source": [
    "### Plots (After)\n",
    "\n",
    "Let's see whether the normalisation improved things\n",
    "\n",
    "#### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932a0f40-81d4-45d1-8e33-a834b1f6b13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = data_numeric_log2_normalised.stack(list(data_numeric_log2.columns.names)).copy()\n",
    "_df.name = 'log2_intensity_normalised'\n",
    "_df = _df.reset_index()\n",
    "\n",
    "_df['is_histone'] = False\n",
    "_df.loc[_df['Label'].isin(HISTONES), 'is_histone'] = True\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10*FIVE_MM_IN_INCH, 15*FIVE_MM_IN_INCH))\n",
    "ax = plt.gca()\n",
    "\n",
    "sns.boxplot(\n",
    "    y='Experiment_Replicate', x='log2_intensity_normalised', \n",
    "    data=_df, hue='is_histone', showfliers=False,\n",
    "    order=data_numeric_log2.columns,\n",
    ")\n",
    "\n",
    "\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), title=\"Histone protein\")\n",
    "ax.set_xlabel(\"Normalised log2 intensity\")\n",
    "ax.set_ylabel(\"Experiment, Replicate\")\n",
    "ax.set_title(\"Distribution of intensities, log2-transformed,\\nnormalised, unfiltered data\")\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / '02-normalisation-boxplot-post-normalisation.pdf'\n",
    "_caption = \"\"\"\n",
    "Boxplot showing the distribution of signal intensities for non-histone proteins (green), and the histone proteins (orange), after  normalisation by the mean of histone protein intensities.\n",
    "The intensity is plotted on the X axis, while the samples are facetted on the Y. The outliers (\"fliers\") are hidden.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55024a9d-0588-4487-a804-1c855e3bfeee",
   "metadata": {},
   "source": [
    "#### Histone heatmap\n",
    "\n",
    "Let's replot histones after normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52c0799-d4a3-417b-ab95-1df897b7f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "_cmap = sns.clustermap(\n",
    "    data_numeric_log2_normalised.loc[HISTONES],\n",
    "    cmap='viridis', robust=True, \n",
    "    figsize=(20*FIVE_MM_IN_INCH, 10*FIVE_MM_IN_INCH),\n",
    "    metric='correlation',\n",
    "    method='complete',\n",
    ")\n",
    "_cmap.ax_col_dendrogram.set_title(\"Normalised data (log2) for histone proteins only\")\n",
    "\n",
    "for tick in _cmap.ax_heatmap.get_yticklabels():\n",
    "    tick_text = tick.get_text()\n",
    "    if tick_text in NORMALISATION_PROTEINS:\n",
    "        tick.set_color('red')\n",
    "        \n",
    "_fname = OUTPUT_DIRECTORY / '02-normalisation-histones-heatmap-post-normalisation.pdf'\n",
    "_caption = \"\"\"\n",
    "Boxplot showing the unnormalised log2 data for histone proteins only.\n",
    "The proteins are plotted on the Y axis, and the experiments on the X.\n",
    "\n",
    "Proteins selected as normalisation factors are highlighted in red.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d2312-1969-4ae4-a7d2-deafe05d2289",
   "metadata": {},
   "source": [
    "#### MA\n",
    "\n",
    "And the MA plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d604e5d-f224-4d82-bd2a-3fba46d7b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric_log2_normalised_row_averages = data_numeric_log2_normalised.mean(axis=1)\n",
    "data_numeric_log2_normalised_row_averages.name = 'avg'\n",
    "data_numeric_log2_normalised_row_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398728c-91c6-46cf-90f6-13d79427ecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric_log2_normalised_diffs = data_numeric_log2_normalised.sub(data_numeric_log2_normalised_row_averages, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf5822-666d-4d78-8866-639dab3fa5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric_log2_normalised_ma = data_numeric_log2_normalised_diffs.stack()\n",
    "data_numeric_log2_normalised_ma.name = 'log2_diff'\n",
    "data_numeric_log2_normalised_ma = data_numeric_log2_normalised_ma.reset_index().join(data_numeric_log2_normalised_row_averages, on='Label')\n",
    "data_numeric_log2_normalised_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4057e88-97e4-4bcf-8ff5-062a587c6eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgrid = sns.FacetGrid(col='Experiment_Replicate', col_wrap=3,\n",
    "                      size=FIVE_MM_IN_INCH*10,\n",
    "                      col_order=data_numeric_log2.columns, data=data_numeric_log2_normalised_ma)\n",
    "fgrid.map(sns.scatterplot, 'avg', 'log2_diff', alpha=.4, color='black')\n",
    "fgrid.map_dataframe(annotate_histones, 'avg', 'log2_diff')\n",
    "# fgrid.map_dataframe(add_horizontal_line)\n",
    "fgrid.set_titles('{col_name}')\n",
    "for experiment, ax in fgrid.axes_dict.items():\n",
    "    ax.axhline(0, linestyle='-', color='red', zorder=1)\n",
    "    ax.grid(False)\n",
    "\n",
    "plt.suptitle(\"MA plots of of log2-transformed, normalised, and unfiltered data\", y=1.05)\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / '02-normalisation-log2-transformed-normalised-unfiltered-ma-plot.pdf'\n",
    "_caption = \"\"\"\n",
    "MA-like plot for log2-transformed, normalised and unfiltered data.\n",
    "\n",
    "X axis plots mean(log2_intensity) across data rows (proteins). X axis is the same in every plot.\n",
    "Y axis plots log2_intensity - mean(log2_intensity) offsets for individual samples\n",
    "\n",
    "Histone proteins are highlighted in green.\n",
    "\n",
    "Red line indicates x=0 line around which the data is centered now.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3601afa9-c57e-44e5-aeea-662630ead1bc",
   "metadata": {},
   "source": [
    "#### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50b5d13-4004-42a0-9795-512801a28e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(data_numeric_log2_normalised.corr(), \n",
    "               cmap='viridis', \n",
    "               annot=True, fmt='.2f',\n",
    "               figsize=(FIVE_MM_IN_INCH*20, FIVE_MM_IN_INCH*20))\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / '02-correlation-matrix-of-normalised-data.pdf'\n",
    "_caption = \"\"\"\n",
    "A Pearson correlation matrix showing the coerrelation coefficient estimate,\n",
    "for the datasets, after normalisation. \n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2003e1-f651-4537-8bc6-4f3f48cd4891",
   "metadata": {},
   "source": [
    "#### PCA\n",
    "\n",
    "We can also observe how the PCA plot changes after the normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0790d542-3d54-4a73-bdb9-bc7c44f5d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adjustText import adjust_text\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Note that we only look at the rows that are all not null:\n",
    "projected = pca.fit_transform(data_numeric_log2_normalised.dropna(axis=0).T)\n",
    "\n",
    "projected = pd.DataFrame(projected, index=data_numeric_log2_normalised.columns, columns=['PC1', 'PC2'])\n",
    "explained_variance = pd.Series(pca.explained_variance_ratio_, index=projected.columns)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Experiment', data=projected.join(headers))\n",
    "\n",
    "texts = []\n",
    "for ix, row in projected.iterrows():\n",
    "    texts.append(ax.text(row['PC1'], row['PC2'], ix, ha='center', va='center'))\n",
    "\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Increasing ylim helps place the H3K4me3 labels\n",
    "ax.set_ylim(ylim[0]*1.1, ylim[1] * 1.1)\n",
    "\n",
    "adjust_text(texts, projected['PC1'].values, projected['PC2'].values, arrowprops=dict(arrowstyle='-'))\n",
    "\n",
    "ax.set_xlabel(\"PC1 ({:.2%} variance)\".format(explained_variance.loc['PC1']))\n",
    "ax.set_ylabel(\"PC2 ({:.2%} variance)\".format(explained_variance.loc['PC2']))\n",
    "ax.grid(False)\n",
    "ax.set_title(\"PCA, log2-transformed data, unfiltered,\\nnormalised, only proteins with no missing values\")\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Experiment')\n",
    "\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / '02-normalisation-PCA-post-normalisation.pdf'\n",
    "_caption = \"\"\"\n",
    "Principal Component embedding of log2-transformed, normalised, and unfiltered dataset, including only the rows which have no missing values.\n",
    "The principal component 1 (PC1) is plotted on the X axis, while the PC2 is plotted on the Y. \n",
    "The explained variance ratio is highlighted in parentheses.\n",
    "\n",
    "Note that the experiment types (colour, see legend on the right) cluster together much better after the normalisation.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0278551-5344-4d63-b777-a5d321d6c8a8",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "It is counterproductive to throw _all_ of the data into the model.\n",
    "For better performance we will filter out some of the proteins.\n",
    "\n",
    "Particularly, we will only keep the proteins which have at least `MINIMUM_NUMBER_OF_OBSERVED_VALUES_PER_CONDITION` in any of the four experimental conditions (H3, H4, H3K4me1, or H3K4me3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb5eb23-7078-4f2b-bf2d-bdaac800ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMUM_NUMBER_OF_OBSERVED_VALUES_PER_CONDITION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf23368-c0df-4648-a0db-773cb982e34f",
   "metadata": {},
   "source": [
    "First count how many non-null values does each protein have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfed27d-d609-4e1a-8c3c-0b2333be494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a long-format DF with non-null values\n",
    "_df = (~data_numeric_log2_normalised.isnull()).stack()\n",
    "_df.name = 'is_not_null'\n",
    "\n",
    "# Join with headers\n",
    "_df = _df.reset_index().join(headers, on='Experiment_Replicate')\n",
    "\n",
    "# Groupby label and experiment and count\n",
    "number_non_null_per_condition = _df.groupby(['Label', 'Experiment'])['is_not_null'].sum().unstack('Experiment')\n",
    "number_non_null_per_condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8cd53e-080f-477d-8a86-aa4d4f795d85",
   "metadata": {},
   "source": [
    "Now check which labels match our condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5bc5c-a6ab-4d51-998c-74ff4674c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sufficient_number_of_values = (number_non_null_per_condition >= MINIMUM_NUMBER_OF_OBSERVED_VALUES_PER_CONDITION).any(axis=1)\n",
    "sufficient_number_of_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c811a9-766c-478c-a805-e58b141f5106",
   "metadata": {},
   "source": [
    "We will throw away all of the proteins which have the value set to False, in hte series above. There are this many of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da62c7-0d31-4ef0-a95c-bd54472430cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sufficient_number_of_values.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b73fde-d732-4346-a4a2-1d878dfc1203",
   "metadata": {},
   "source": [
    "In order to stay civilised we will create a new series called \"Comment\" and mark the reason of filtering out in there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e96637-44cc-44db-af77-31406a6ef25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comment = pd.Series(None, index=data_numeric_log2_normalised.index, name='Comment', dtype=str)\n",
    "data_comment.loc[~sufficient_number_of_values] = \"Insufficient number of non-null values\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d18a896-f613-40d4-85ad-77a6e2477f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13d282a-9a88-4062-abba-462aefd3747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bcfb39-260c-43de-9876-8af318629dd3",
   "metadata": {},
   "source": [
    "Perfect, we can move on to modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05220d81-7851-49fe-b9e9-aa91648ecd99",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664c7b74-6d55-4714-bf89-b485e4e58801",
   "metadata": {},
   "source": [
    "### The inputs\n",
    "\n",
    "We will model the only the normalised data which has a sufficient number of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2403adde-7a2b-4a21-a11d-b5b1e31add6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_model = data_numeric_log2_normalised.loc[sufficient_number_of_values]\n",
    "data_to_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e3c741-f673-437d-b9ad-84641b8618b7",
   "metadata": {},
   "source": [
    "This is how the missing value distribution for this data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa2bf55-a19e-4fce-9340-8deb4e73b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msgno\n",
    "msgno.matrix(data_to_model, figsize=(20*FIVE_MM_IN_INCH, 20*FIVE_MM_IN_INCH), fontsize=6)\n",
    "\n",
    "plt.title(\"Distribution of missing values in log2-transformed, normalised, and filtered data\")\n",
    "plt.xlabel(\"Data columns (experiments)\")\n",
    "plt.ylabel(\"Data rows (proteins)\")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / '03-modelling-log2-transformed-normalised-filtered-missing-values.png'\n",
    "_caption = \"\"\"\n",
    "Distribution of missing values in the log2-transformed, normalised, and filtered data which will be used for modelling. \n",
    "The heatmap above displays all  929 proteins remaining in the data after filtering in rows, and all experiments in columns. \n",
    "Dark values indicate the presence of a value, while white space indicates the absence of one. \n",
    "The sparkline on the right hand side counts the number of values per protein, which ranges from zero to 12 in this dataset.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b71967e-156d-4bb8-8f7c-9be4a53197e1",
   "metadata": {},
   "source": [
    "The special cases here are the proteins which have all zero values in some of the conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef90bcd-a0be-4935-b834-b01061341070",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_zero_in_some_condition = (number_non_null_per_condition == 0).any(axis=1)\n",
    "all_zero_in_some_condition = all_zero_in_some_condition.loc[data_to_model.index]\n",
    "all_zero_in_some_condition.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7858447a-d5db-4663-b87f-4d66fba32a47",
   "metadata": {},
   "source": [
    "These proteins are special cases. To visualise them better let's establish a consistent sort order\n",
    "which is based on which condition is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6614494a-1644-45b1-a485-411372f41bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_zero_in_some_condition_sort_order = number_non_null_per_condition.loc[all_zero_in_some_condition[all_zero_in_some_condition].index].sort_values(by=['H3', 'H4', 'H3K4me1', 'H3K4me3']).index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caacb861-a8da-462c-9d86-aa68c4c95d2b",
   "metadata": {},
   "source": [
    "Let's plot these special cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e5e4b-e896-435e-b272-7dabe793a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msgno\n",
    "msgno.matrix(\n",
    "    data_to_model.loc[all_zero_in_some_condition_sort_order], figsize=(20*FIVE_MM_IN_INCH, 20*FIVE_MM_IN_INCH), fontsize=6)\n",
    "\n",
    "plt.title(\"Distribution of missing values in log2-transformed, normalised, and filtered data\\nshown only the special cases with all-nulls in some of the conditions\")\n",
    "plt.xlabel(\"Data columns (experiments)\")\n",
    "plt.ylabel(\"Data rows (proteins)\")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# For some reason this plot does not rasterize well..\n",
    "_fname = OUTPUT_DIRECTORY / '03-modelling-log2-transformed-normalised-filtered-missing-values-all_zero_in_some_condition.png'\n",
    "_caption = \"\"\"\n",
    "Distribution of missing values in the log2-transformed, normalised, and filtered data of proteins that correspond to \"special cases\" in the modeling as they have all zero values in at least one of the conditions.\n",
    "The heatmap above displays all 102 of such proteins, sorted by their missing value pattern. \n",
    "Dark values indicate the presence of a value, while white space indicates the absence of one. \n",
    "The sparkline on the right hand side counts the number of values per protein, which ranges from zero to 12 in this dataset.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10681305-395f-4c2a-a75a-86a5c58ea9d0",
   "metadata": {},
   "source": [
    "We can also get a few special cases of proteins from that list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e4977-080d-4b25-a6df-e57ada6fed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_non_null_per_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a980da-e688-4474-85b5-f26c7a822773",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_non_null_per_condition.loc[\n",
    "    (number_non_null_per_condition[['H3', 'H4']] > 2).any(axis=1) & (number_non_null_per_condition[['H3K4me1', 'H3K4me3']] == 0).all(axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbbb1f2-41cf-46ae-a130-a8e385be96fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPRESENTATIVE_NULLS = [\n",
    "    'FAM98A', 'PHF8', 'EIF4G1', 'ZMYND11', # Controls null, proteins non-null\n",
    "    'PSMC1', 'SNTB2', 'TUBB2A', 'TNNT3', # Controls non-null, proteins null\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a9fd72-6c54-4394-b991-cfd958addf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to long format\n",
    "_df = data_to_model.copy()\n",
    "_df = _df.stack(_df.columns.names)\n",
    "_df.name = 'value'\n",
    "_df = _df.reset_index()\n",
    "\n",
    "# Plot the histograms and normal distribution fits\n",
    "fgrid = sns.FacetGrid(col='Experiment_Replicate', col_wrap=3, col_order=data_to_model.columns, data=_df, size=FIVE_MM_IN_INCH*5)\n",
    "fgrid.map(sns.histplot, 'value', stat='density')\n",
    "fgrid.map(_plot_norm_fit, 'value')\n",
    "fgrid.set_titles('{col_name}')\n",
    "\n",
    "plt.suptitle(\"Normal fit of log2-transformed, normalised, and filtered data\", y=1.05)\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / '03-modelling-log2-transformed-normalised-filtered-normal-fit.pdf'\n",
    "_caption = \"\"\"\n",
    "Distribution of non-null values of log2-transformed, normalised, and filtered dataset. \n",
    "The rows facet the four experiment types, while the columns split the data per replicate.\n",
    "The green distribution plot shows the histogram of observed log2 values in the data.\n",
    "The black curve shows the corresponding normal distribution fit to the data, whose parameters are written in the top-right corner.\n",
    "The black vertical line indicates the mean ($\\mu$) estimate.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b21e59-2490-40e7-8aa9-1b19e692947e",
   "metadata": {},
   "source": [
    "### Design matrix and contrasts\n",
    "\n",
    "For modeling we will use limma and encode the data as a \"means\" model.\n",
    "\n",
    "We first need to load the limma into `rpy2` environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b7831b-68d8-4fec-a72f-7344f64a4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri, numpy2ri\n",
    "\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78abfb0a-4666-4d57-a664-1de21146caba",
   "metadata": {},
   "source": [
    "Time to load data into R, we will use `limma` for most tastks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32287991-149e-44d7-b4d2-5b720f07f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(\"limma\")\n",
    "packageVersion(\"limma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ed495-bb73-440e-b92f-fd2c76ee39cb",
   "metadata": {},
   "source": [
    "Now we load the header data and make sure that it's columns are interpreted as factors correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a2d06-a16e-4e6d-a873-3c6dcab8a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert headers.index.equals(data_to_model.columns), \"Make sure that headers index matches data_to_model columns!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45be1c86-4898-4b10-b6a2-5693341c0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i headers\n",
    "\n",
    "\n",
    "headers$Experiment <- as.factor(headers$Experiment)\n",
    "headers$Experiment <- relevel(headers$Experiment, ref=\"H4\")\n",
    "\n",
    "headers$Replicate <- as.factor(headers$Replicate)\n",
    "headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b5237e-def1-479c-813b-3f5155bd0356",
   "metadata": {},
   "source": [
    "To create the design matrix, we will use a Means (no intercept) design "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f652918a-b3de-437b-90cf-d5c8a66dee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Do not use intercept:\n",
    "design <- model.matrix(~0 + Experiment, headers)\n",
    "design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a286f-a6a8-4027-9c09-9bb98bc60e55",
   "metadata": {},
   "source": [
    "From this design matrix we are primarily interested in three contrasts:\n",
    "\n",
    "- `H3K4me1vsControl`: `H3K4me1` vs mean of `H4` and `H3` controls\n",
    "- `H3K4me3vsControl`: `H3K4me3` vs mean of `H4` and `H3` controls\n",
    "- `H3K4me3vsH3K4me1`: `H3K4me3` vs `H3K4me1` (note that this is the same as `H3K4me1vsControl` vs `H3K4me3vsControl`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0bef77-c94e-484d-b9b9-2b47be16fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -o contrasts_matrix_as_df\n",
    "\n",
    "contrasts.matrix <- limma::makeContrasts(\n",
    "    \"H3K4me1vsControl\"=ExperimentH3K4me1-(ExperimentH4 + ExperimentH3)/2,\n",
    "    \"H3K4me3vsControl\"=ExperimentH3K4me3-(ExperimentH4 + ExperimentH3)/2, \n",
    "    \"H3K4me3vsH3K4me1\"=ExperimentH3K4me3-ExperimentH3K4me1, \n",
    "    \n",
    "    levels=design\n",
    ")\n",
    "\n",
    "contrasts_matrix_as_df = as.data.frame(contrasts.matrix)\n",
    "contrasts.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b0ae2d-db3b-4bed-bc7d-1c0756e9ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrasts_matrix_as_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32522972-fa35-40ce-b561-e1e47d76de43",
   "metadata": {},
   "source": [
    "### The fit\n",
    "\n",
    "Here we fit the simple limma model, and extract the resulting coefficient estimates and contrasts.\n",
    "Note that we use `robust=TRUE` in the `eBayes` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a553001c-6fa5-42fa-be4f-e5994edaac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i data_to_model -o coef_estimates_design -o coef_estimates_contrasts\n",
    "\n",
    "\n",
    "fit <- limma::lmFit(data_to_model, design)\n",
    "\n",
    "coef_estimates_design = as.data.frame(fit$coefficients)\n",
    "\n",
    "fit2 <- limma::contrasts.fit(fit, contrasts.matrix)\n",
    "\n",
    "# Note robust=TRUE\n",
    "fit2 <- limma::eBayes(fit2, robust=TRUE)\n",
    "\n",
    "coef_estimates_contrasts <- as.data.frame(fit2$coefficients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd376e4-340b-45f8-a97e-feb6c61feb2c",
   "metadata": {},
   "source": [
    "For coefficient estimates it is now most important to check that the coefficients were estimated in a predictable way for those special case proteins that have all nulls in at least one of the conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c57f7-e65b-4dc0-9b7c-ab12323c57fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = number_non_null_per_condition.loc[data_to_model.index]\n",
    "\n",
    "fraction_null_coefs = []\n",
    "# For each experiment\n",
    "for experiment in _df.columns:\n",
    "    # For each number of non-null values possible\n",
    "    for non_null_count, _subdf in _df[experiment].groupby(_df[experiment]):\n",
    "        # Count number of null coefficient_estimates for the coefficient\n",
    "        fraction_null_coefs.append([experiment, non_null_count, coef_estimates_design.loc[_subdf.index, f'Experiment{experiment}'].isnull().astype(float).mean()])\n",
    "        \n",
    "fraction_null_coefs = pd.DataFrame(fraction_null_coefs, columns=['Experiment',' Number of non-null observations', 'Fraction of null coefficient estimates'])\n",
    "fraction_null_coefs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048de8bb-93f4-4a6d-abb6-22c44c5d275a",
   "metadata": {},
   "source": [
    "Note that the coefficients are always null when there is zero data for this particular condition (fraction=1)\n",
    "But always non-null otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c08d17-fdec-4576-95d5-6cef2e4378fc",
   "metadata": {},
   "source": [
    "Below is avisualisation of a few representative proteins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c270d5-7edc-455c-a316-272d263601a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_model.loc[REPRESENTATIVE_NULLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d0d46-25ca-4b50-a47a-736948c565d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_estimates_design.loc[REPRESENTATIVE_NULLS, ['ExperimentH3', 'ExperimentH4', 'ExperimentH3K4me3', 'ExperimentH3K4me1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e952d-7440-4248-9fbb-932d2cb55ca5",
   "metadata": {},
   "source": [
    "And the corresponding contrasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae79084-194f-488d-88ab-de8915d3af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_estimates_contrasts.loc[REPRESENTATIVE_NULLS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff58d16-ce05-4c75-9a9e-fdf32676361f",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14465eb-decc-4ca4-b42d-081d2e4f39c1",
   "metadata": {},
   "source": [
    "The results can be extracted from R by calling the `topTable` function in limma once for each contrast, and setting `number` parameter to the number of rows in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78654755-15c2-49a1-962e-d131f4fb8d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -o results_r\n",
    "\n",
    "coefs <- colnames(contrasts.matrix)\n",
    "results_r <- list()\n",
    "\n",
    "for (i in 1:length(coefs)) {\n",
    "    coef <- coefs[[i]]\n",
    "    results_r[[coef]] <- limma::topTable(fit2, coef=coef, adjust.method=\"BH\", number=nrow(data_to_model))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f00fec9-7370-402d-bb77-6f916c5d66d1",
   "metadata": {},
   "source": [
    "We convert these results to python and add another column called \"Significant\" which is true if the adjusted p value is lower than `FDR_THRESHOLD` specified in parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305fdc9a-c8e8-4788-9c68-915b8c9b2d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FDR_THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9b2a57-7423-444e-b55b-0b25d3598659",
   "metadata": {},
   "outputs": [],
   "source": [
    "with robjects.conversion.localconverter(robjects.default_converter + pandas2ri.converter) as co:\n",
    "    results = {\n",
    "        k: co.rpy2py(v) for k,v in results_r.items()\n",
    "    }\n",
    "\n",
    "for k in results.keys():\n",
    "    results[k]['significant'] = results[k]['adj.P.Val'] <= FDR_THRESHOLD\n",
    "    \n",
    "results = pd.concat(results, keys=results.keys(), axis=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab45f79-8e02-4b59-b937-b236ebc711c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(results.index) == set(data_to_model.index), \"Results index does not match input index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b04e6-ff25-4bbe-96fc-c57225adce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc(axis=1)[:, 'significant'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc123c2-bb03-4a46-8d32-7d8b619938a2",
   "metadata": {},
   "source": [
    "### Sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2e2fc-c288-4054-9bce-dd150c8f4e31",
   "metadata": {},
   "source": [
    "#### Intuitive interpretation of contrasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d664dc7-a29c-4bc6-9e59-1b8df7b407f8",
   "metadata": {},
   "source": [
    "We now check these results to see that the estimated log fold changes (by limma) are close to what we would expect from naive modelling. \n",
    "\n",
    "This is needed both as a sanity check, and as a justification for \"fold change imputation\" for a few cases where the FCs could not be estimated analytically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab970f-44e2-4ab9-a50c-cf263c3d1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_model_by_experiment = {\n",
    "    'H3': data_to_model[['H3_1', 'H3_2', 'H3_3']],\n",
    "    'H4': data_to_model[['H4_1', 'H4_2', 'H4_3']],\n",
    "    'H3K4me1': data_to_model[['H3K4me1_1', 'H3K4me1_2', 'H3K4me1_3']],\n",
    "    'H3K4me3': data_to_model[['H3K4me3_1', 'H3K4me3_2', 'H3K4me3_3']],\n",
    "    'Controls': data_to_model[['H3_1', 'H3_2', 'H3_3', 'H4_1', 'H4_2', 'H4_3']],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b85c0-e3b8-497f-890a-f7c3b87c583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_controls = 0.5 * (data_to_model_by_experiment['H3'].mean(axis=1) +  data_to_model_by_experiment['H4'].mean(axis=1))\n",
    "expected_naively = {\n",
    "    'H3K4me1vsControl': data_to_model_by_experiment['H3K4me1'].mean(axis=1) - _controls,\n",
    "    'H3K4me3vsControl': data_to_model_by_experiment['H3K4me3'].mean(axis=1) - _controls,\n",
    "    'H3K4me3vsH3K4me1': data_to_model_by_experiment['H3K4me3'].mean(axis=1) - data_to_model_by_experiment['H3K4me1'].mean(axis=1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828bda9a-e7b8-412f-8e8b-c1678ba703e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010cda6b-15b9-465c-b8c1-2954cc84d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_naively['H3K4me3vsH3K4me1'].loc['PHF8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c48e7-4ded-4db8-a6f3-97b1d13c8b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['H3K4me3vsH3K4me1', 'logFC'].loc['PHF8']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6305c65-d587-499c-862a-a3cbaf339978",
   "metadata": {},
   "source": [
    "import pingouin as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcdc4c2-6e3d-4ae5-8380-127435c056b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e707ee6-2929-4594-9565-dd91dc5d6af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column, expectation in expected_naively.items():\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    _df = pd.DataFrame({'expectation': expectation, 'actual': results[column, 'logFC']})\n",
    "    \n",
    "    correlation = pg.corr(_df['expectation'], _df['actual'], method='pearson')\n",
    "    assert correlation.loc['pearson', 'r'] == 1 # This should always be one\n",
    "    correlation = correlation.loc['pearson', 'CI95%']\n",
    "    \n",
    "    sns.regplot(x='expectation', y='actual', scatter_kws=dict(alpha=0.2), data=_df)\n",
    "    \n",
    "    ax.text(0.02, 0.98, r'$r \\in [{}, {}]$'.format(*correlation), transform=ax.transAxes, ha='left', va='top')\n",
    "    ax.set_xlabel(\"Expected result (calculated naively)\")\n",
    "    ax.set_ylabel(\"Actual result (limma)\")\n",
    "    ax.set_title(column)\n",
    "    sns.despine(ax=ax, offset=5)\n",
    "    \n",
    "    ax.grid(False)\n",
    "    \n",
    "    _fname = OUTPUT_DIRECTORY / f'04-modelling-sanity-checks-for-coefficient-interpretation-{column}.pdf'\n",
    "    _caption = f\"\"\"\n",
    "    Comparison of the {column} coefficient estimates computed by the model (y axis), and their expected values (x axis).\n",
    "    The expected values were computed naively - by subtracting the coresponding means from each other.\n",
    "    The actual values were computed using the limma statistical model.\n",
    "    \n",
    "    The scatterpoint highlight individual protein estimates, the line is a linear regression estimate.\n",
    "    The 95% confidence interval for pearson R estimate is written in the top left corner.\n",
    "    \n",
    "    If the model is working correctly, we would expect a perfect fit here.\n",
    "    \"\"\"\n",
    "    plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "    with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "        f.write(_caption)\n",
    "        print(_caption)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50a66c7-2de1-4b94-92d9-20dd91063545",
   "metadata": {},
   "source": [
    "It's also worth seeing how much these results would change if we took a naive mean of the controls, i.e. mean(H3_1,H3_2,H3_3,H4_1,H4_2,H4_3) as opposed to the mean-of-means approach, i.e. 0.5 (mean(H3_1,H3_2,H3_3) + mean(H4_1,H4_2,H4_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f7ca1-a404-439b-b82f-528b928f1ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_controls_simple = data_to_model_by_experiment['Controls'].mean(axis=1)\n",
    "expected_naively_simple_mean_of_controls = {\n",
    "    'H3K4me1vsControl': data_to_model_by_experiment['H3K4me1'].mean(axis=1) - _controls_simple,\n",
    "    'H3K4me3vsControl': data_to_model_by_experiment['H3K4me3'].mean(axis=1) - _controls_simple,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ce4bc-7383-4c0f-8b00-9d3ad522c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column, expectation in expected_naively_simple_mean_of_controls.items():\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    _df = pd.DataFrame({'expectation': expectation, 'actual': results[column, 'logFC']})\n",
    "    \n",
    "    correlation = pg.corr(_df['expectation'], _df['actual'], method='pearson').loc['pearson', 'CI95%']\n",
    "    \n",
    "    sns.regplot(x='expectation', y='actual', scatter_kws=dict(alpha=0.2), data=_df)\n",
    "    \n",
    "    ax.text(0.02, 0.98, r'$r \\in [{}, {}]$'.format(*correlation), transform=ax.transAxes, ha='left', va='top')\n",
    "    ax.set_xlabel(\"Expected result (calculated naively, using simple mean of controls)\")\n",
    "    ax.set_ylabel(\"Actual result (limma)\")\n",
    "    ax.set_title(column)\n",
    "    sns.despine(ax=ax, offset=5)\n",
    "    \n",
    "    ax.grid(False)\n",
    "    \n",
    "    _fname = OUTPUT_DIRECTORY / f'04-modelling-sanity-checks-for-coefficient-interpretation-{column}-simple-mean-of-controls.pdf'\n",
    "    _caption = f\"\"\"\n",
    "    Comparison of the {column} coefficient estimates computed by the model (y axis), and their simplified expected values (x axis).\n",
    "    The expected values were computed naively - by subtracting the coresponding means from each other, and by taking a simple mean of the controls, as opposed to the mean-of-means used in the model.\n",
    "    The actual values were computed using the limma statistical model.\n",
    "    \n",
    "    The scatterpoint highlight individual protein estimates, the line is a linear regression estimate.\n",
    "    The 95% confidence interval for pearson R estimate is written in the top left corner.\n",
    "    \n",
    "    If the model is working correctly, we would expect a perfect fit here.\n",
    "    \"\"\"\n",
    "    plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "    with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "        f.write(_caption)\n",
    "        print(_caption)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0084f0d-5f8a-456b-ab1c-ff7dea3c5b7c",
   "metadata": {},
   "source": [
    "#### Heatmaps\n",
    "\n",
    "##### H3K4me1 vs Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df448c4d-3cdc-46f7-90ee-69c34a32c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "_colname = 'H3K4me1vsControl'\n",
    "_significant = results.loc[results[_colname, 'significant'], _colname].sort_values(by='logFC', ascending=False)\n",
    "_significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837458f8-bccc-41f3-ab36-bc9ffb49cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "figure = plt.figure(figsize=(20*FIVE_MM_IN_INCH, 28*FIVE_MM_IN_INCH), constrained_layout=True)\n",
    "\n",
    "gs = GridSpec(2, 2, figure=figure, width_ratios=(1, 12), height_ratios=(10,1))\n",
    "ax_legends = figure.add_subplot(gs[1,:])\n",
    "ax_legends.axis('off')\n",
    "cax_left = ax_legends.inset_axes([0.02, 0.58, 0.2, 0.2], transform=ax_legends.transAxes)\n",
    "cax_right = ax_legends.inset_axes([0.32, 0.58, 0.2, 0.2], transform=ax_legends.transAxes)\n",
    "\n",
    "cax_left.set_title(\"LogFC estimate\")\n",
    "cax_right.set_title(\"Normalised data,\\nH3/H4 centred\")\n",
    "\n",
    "ax_left = figure.add_subplot(gs[0, 0])\n",
    "ax_right = figure.add_subplot(gs[0, 1], sharey=ax_left)\n",
    "\n",
    "\n",
    "vmax = np.ceil(_significant['logFC'].abs().replace(np.inf, np.nan).dropna().quantile(0.97))\n",
    "vmin = -vmax\n",
    "\n",
    "_matrix = data_to_model.sub(data_to_model_by_experiment['Controls'].mean(axis=1), axis=0)\n",
    "_matrix = _matrix.loc[_significant.index]\n",
    "\n",
    "heatmap_right = sns.heatmap(\n",
    "    _matrix, \n",
    "    cmap='RdBu_r', robust=True, center=0,\n",
    "    ax=ax_right, yticklabels=0, \n",
    "    cbar=True,\n",
    "    cbar_ax = cax_right,\n",
    "    cbar_kws=dict(orientation='horizontal'),\n",
    "    linewidth=0,\n",
    "    linecolor='black',\n",
    ")\n",
    "\n",
    "heatmap_right.set_facecolor('#969696')\n",
    "\n",
    "heatmap_left = sns.heatmap(\n",
    "    _significant[['logFC']], \n",
    "    cmap='RdBu_r', \n",
    "    center=0, \n",
    "    robust=True,\n",
    "    ax=ax_left, yticklabels=0, \n",
    "    cbar=True,\n",
    "    cbar_ax=cax_left,\n",
    "    cbar_kws=dict(orientation='horizontal'),\n",
    "    linewidth=0,\n",
    "    vmin=vmin, vmax=vmax,\n",
    "    linecolor='black',\n",
    ")\n",
    "\n",
    "heatmap_right.xaxis.tick_top()\n",
    "heatmap_right.xaxis.set_tick_params(length=0, rotation=90)\n",
    "\n",
    "heatmap_left.xaxis.tick_top()\n",
    "heatmap_left.xaxis.set_tick_params(length=0, rotation=90)\n",
    "\n",
    "heatmap_right.yaxis.set_tick_params(length=0)\n",
    "heatmap_left.yaxis.set_tick_params(length=0)\n",
    "\n",
    "for ax in [heatmap_left, heatmap_right]:\n",
    "    for spine in ax.spines:\n",
    "        ax.spines[spine].set_visible(True)\n",
    "\n",
    "for tick in heatmap_right.yaxis.get_ticklabels():\n",
    "    tick.set_visible(False)\n",
    "\n",
    "heatmap_right.set_title(f\"Proteins with significant logFC estimates for {_colname}\")\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / f'05-modelling-heatmap-of-significant-proteins-for-{_colname}.pdf'\n",
    "_caption = f\"\"\"\n",
    "\n",
    "Heatmap of the significant `logFC` estimates for the {_colname} coefficient.\n",
    "The proteins are sorted by the estimate, descending.\n",
    "\n",
    "The left heatmap plots the `logFC` estimate, the colour axis limits are set to [{vmin}, {vmax}].\n",
    "The right heatmap shows the data used for modelling, standardised to have zero mean of controls (H3 and H4). \n",
    "\n",
    "Missing values are marked in grey.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845db7f6-9c53-4545-972f-1bde76a2efcf",
   "metadata": {},
   "source": [
    "##### H3K4me3 vs Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5643c3-0cc2-4ad2-a585-01a2c96c686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_colname = 'H3K4me3vsControl'\n",
    "_significant = results.loc[results[_colname, 'significant'], _colname].sort_values(by='logFC', ascending=False)\n",
    "_significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f37b29-bf81-42b3-8c70-d7756a76a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "figure = plt.figure(figsize=(20*FIVE_MM_IN_INCH, 28*FIVE_MM_IN_INCH), constrained_layout=True)\n",
    "\n",
    "gs = GridSpec(2, 2, figure=figure, width_ratios=(1, 12), height_ratios=(10,1))\n",
    "ax_legends = figure.add_subplot(gs[1,:])\n",
    "ax_legends.axis('off')\n",
    "cax_left = ax_legends.inset_axes([0.02, 0.58, 0.2, 0.2], transform=ax_legends.transAxes)\n",
    "cax_right = ax_legends.inset_axes([0.32, 0.58, 0.2, 0.2], transform=ax_legends.transAxes)\n",
    "\n",
    "cax_left.set_title(\"LogFC estimate\")\n",
    "cax_right.set_title(\"Normalised data,\\nH3/H4 centred\")\n",
    "\n",
    "ax_left = figure.add_subplot(gs[0, 0])\n",
    "ax_right = figure.add_subplot(gs[0, 1], sharey=ax_left)\n",
    "\n",
    "\n",
    "vmax = np.ceil(_significant['logFC'].abs().replace(np.inf, np.nan).dropna().quantile(0.97))\n",
    "vmin = -vmax\n",
    "\n",
    "_matrix = data_to_model.sub(data_to_model_by_experiment['Controls'].mean(axis=1), axis=0)\n",
    "_matrix = _matrix.loc[_significant.index]\n",
    "\n",
    "heatmap_right = sns.heatmap(\n",
    "    _matrix, \n",
    "    cmap='RdBu_r', robust=True, center=0,\n",
    "    ax=ax_right, yticklabels=0, \n",
    "    cbar=True,\n",
    "    cbar_ax = cax_right,\n",
    "    cbar_kws=dict(orientation='horizontal'),\n",
    "    linewidth=0,\n",
    "    linecolor='black',\n",
    ")\n",
    "heatmap_right.set_facecolor('#969696')\n",
    "\n",
    "heatmap_left = sns.heatmap(\n",
    "    _significant[['logFC']], \n",
    "    cmap='RdBu_r', \n",
    "    center=0, \n",
    "    robust=True,\n",
    "    ax=ax_left, yticklabels=0, \n",
    "    cbar=True,\n",
    "    cbar_ax=cax_left,\n",
    "    cbar_kws=dict(orientation='horizontal'),\n",
    "    linewidth=0,\n",
    "    vmin=vmin, vmax=vmax,\n",
    "    linecolor='black',\n",
    ")\n",
    "\n",
    "heatmap_right.xaxis.tick_top()\n",
    "heatmap_right.xaxis.set_tick_params(length=0, rotation=90)\n",
    "\n",
    "heatmap_left.xaxis.tick_top()\n",
    "heatmap_left.xaxis.set_tick_params(length=0, rotation=90)\n",
    "\n",
    "heatmap_right.yaxis.set_tick_params(length=0)\n",
    "heatmap_left.yaxis.set_tick_params(length=0)\n",
    "\n",
    "for ax in [heatmap_left, heatmap_right]:\n",
    "    for spine in ax.spines:\n",
    "        ax.spines[spine].set_visible(True)\n",
    "\n",
    "for tick in heatmap_right.yaxis.get_ticklabels():\n",
    "    tick.set_visible(False)\n",
    "\n",
    "heatmap_right.set_title(f\"Proteins with significant logFC estimates for {_colname}\")\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / f'05-modelling-heatmap-of-significant-proteins-for-{_colname}.pdf'\n",
    "_caption = f\"\"\"\n",
    "\n",
    "Heatmap of the significant `logFC` estimates for the {_colname} coefficient.\n",
    "The proteins are sorted by the estimate, descending.\n",
    "\n",
    "The left heatmap plots the `logFC` estimate, the colour axis limits are set to [{vmin}, {vmax}].\n",
    "The right heatmap shows the data used for modelling, standardised to have zero mean of controls (H3 and H4). \n",
    "\n",
    "Missing values are marked in grey.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b5fac4-f282-491a-897e-987a5af8f55d",
   "metadata": {},
   "source": [
    "##### H3K4me3 vs H3K4me1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b8194b-32cb-471c-992e-499c10bb7656",
   "metadata": {},
   "outputs": [],
   "source": [
    "_colname = 'H3K4me3vsH3K4me1'\n",
    "_significant = results.loc[results[_colname, 'significant'], _colname].sort_values(by='logFC', ascending=False)\n",
    "_significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0bb37f-8296-4bfa-83ce-562855bfaa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "figure = plt.figure(figsize=(20*FIVE_MM_IN_INCH, 28*FIVE_MM_IN_INCH), constrained_layout=True)\n",
    "\n",
    "gs = GridSpec(2, 2, figure=figure, width_ratios=(1, 12), height_ratios=(10,1))\n",
    "ax_legends = figure.add_subplot(gs[1,:])\n",
    "ax_legends.axis('off')\n",
    "cax_left = ax_legends.inset_axes([0.02, 0.58, 0.2, 0.2], transform=ax_legends.transAxes)\n",
    "cax_right = ax_legends.inset_axes([0.32, 0.58, 0.2, 0.2], transform=ax_legends.transAxes)\n",
    "\n",
    "cax_left.set_title(\"LogFC estimate\")\n",
    "cax_right.set_title(\"Normalised data,\\nminus mean(H3K4me1,H3K4me3)\")\n",
    "\n",
    "ax_left = figure.add_subplot(gs[0, 0])\n",
    "ax_right = figure.add_subplot(gs[0, 1], sharey=ax_left)\n",
    "\n",
    "\n",
    "vmax = np.ceil(_significant['logFC'].abs().replace(np.inf, np.nan).dropna().quantile(0.97))\n",
    "vmin = -vmax\n",
    "\n",
    "_matrix = data_to_model.sub(data_to_model_by_experiment['H3K4me1'].join(data_to_model_by_experiment['H3K4me3']).mean(axis=1), axis=0)\n",
    "_matrix = _matrix.loc[_significant.index]\n",
    "\n",
    "heatmap_right = sns.heatmap(\n",
    "    _matrix, \n",
    "    cmap='RdBu_r', robust=True, center=0,\n",
    "    ax=ax_right, yticklabels=1, \n",
    "    cbar=True,\n",
    "    cbar_ax = cax_right,\n",
    "    cbar_kws=dict(orientation='horizontal'),\n",
    "    linewidth=0.1,\n",
    "    linecolor='black',\n",
    ")\n",
    "heatmap_right.set_facecolor('#969696')\n",
    "heatmap_left = sns.heatmap(\n",
    "    _significant[['logFC']], \n",
    "    cmap='RdBu_r', \n",
    "    center=0, \n",
    "    robust=True,\n",
    "    ax=ax_left, yticklabels=1, \n",
    "    cbar=True,\n",
    "    cbar_ax=cax_left,\n",
    "    cbar_kws=dict(orientation='horizontal'),\n",
    "    linewidth=0.1,\n",
    "    vmin=vmin, vmax=vmax,\n",
    "    linecolor='black',\n",
    ")\n",
    "\n",
    "heatmap_right.xaxis.tick_top()\n",
    "heatmap_right.xaxis.set_tick_params(length=0, rotation=90)\n",
    "\n",
    "heatmap_left.xaxis.tick_top()\n",
    "heatmap_left.xaxis.set_tick_params(length=0, rotation=90)\n",
    "\n",
    "heatmap_right.yaxis.set_tick_params(length=0)\n",
    "heatmap_left.yaxis.set_tick_params(length=0)\n",
    "\n",
    "for ax in [heatmap_left, heatmap_right]:\n",
    "    for spine in ax.spines:\n",
    "        ax.spines[spine].set_visible(True)\n",
    "\n",
    "for tick in heatmap_right.yaxis.get_ticklabels():\n",
    "    tick.set_visible(False)\n",
    "\n",
    "heatmap_right.set_title(f\"Proteins with significant logFC estimates for {_colname}\")\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / f'05-modelling-heatmap-of-significant-proteins-for-{_colname}.pdf'\n",
    "_caption = f\"\"\"\n",
    "\n",
    "Heatmap of the significant `logFC` estimates for the {_colname} coefficient.\n",
    "The proteins are sorted by the estimate, descending.\n",
    "\n",
    "The left heatmap plots the `logFC` estimate, the colour axis limits are set to [{vmin}, {vmax}].\n",
    "The right heatmap shows the data used for modelling, standardised to have mean(H3K4me3 and H3K4me1) = 0. \n",
    "\n",
    "Missing values are marked in grey.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef9c1b0-bba9-4612-add8-5888b8d58306",
   "metadata": {},
   "source": [
    "## Imputation of results\n",
    "\n",
    "While the model behaves correctly for most proteins, we want to:\n",
    "\n",
    "1. flag the following result estimates:\n",
    "   - estimates which are based only on one data point\n",
    "   - imputed estimates (see below)\n",
    "2. impute certain fold change estimates, namely:\n",
    "   - mark proteins detected in treatments, but not detected in both controls as \"infinitely enriched\" (logFC=+inf)\n",
    "   - mark proteins detected in both controls, but not in treatmets as \"infinitely excluded\" (logFC=-inf)\n",
    "   - in cases where logFC could not be estimated because one (but not both!) controls are missing, calculate the logFC only from the control that is detected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8a6ee-001d-47de-bbe9-8c752656944d",
   "metadata": {},
   "source": [
    "Make sure code below aligns with the contrasts matrix here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0860921-54ac-4617-bf99-e03714232a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrasts_matrix_as_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6dde70-3f7c-4aa8-8ae4-4f79cea8258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_and_or_impute(estimate, treatments, controls):\n",
    "    \n",
    "    # Count number of datapoints for treatment and control\n",
    "    number_non_null_treatments = (~treatments.isnull()).sum(axis=1)\n",
    "    number_non_null_controls = (~controls.isnull()).sum(axis=1)\n",
    "    \n",
    "    # First check if our estimate is based on a single datapoint\n",
    "    # This is true if treatments have only one non-null value\n",
    "                                                            \n",
    "    based_on_single_datapoint = number_non_null_treatments == 1\n",
    "    # Or the controls have only one non-null value\n",
    "    based_on_single_datapoint |= number_non_null_controls == 1\n",
    "    \n",
    "    # Now let's see if we need to impute some of the estimates\n",
    "    \n",
    "    \n",
    "    # Start with copying the estimates\n",
    "    imputed_estimate = estimate.copy()\n",
    "    \n",
    "    # Find datapoints w/o estimates\n",
    "    no_estimate = estimate.isnull()\n",
    "    \n",
    "    # Impute positive infinities\n",
    "    imputed_estimate.loc[\n",
    "        # Where we have no estimate, but treatment data and no control data\n",
    "        no_estimate & (number_non_null_treatments > 0)  & (number_non_null_controls == 0),\n",
    "    ] = np.inf\n",
    "    \n",
    "    # Impute negative infinities\n",
    "    imputed_estimate.loc[\n",
    "        # Where we have no estimate, no treatment data but some control data\n",
    "        no_estimate & (number_non_null_treatments == 0)  & (number_non_null_controls > 0),\n",
    "    ] = -np.inf\n",
    "    \n",
    "    # Impute estimates where we have some treatment and control data, but still no estimate\n",
    "    # (this happens when only one of the controls give a value, for instance)\n",
    "    imputable_entries = no_estimate & (number_non_null_treatments > 0)  & (number_non_null_controls > 0)\n",
    "    \n",
    "    # Here we are using the results from the \"sanity checks above\" - that the mean estimates are\n",
    "    # the same as model estimates..\n",
    "    imputed_estimate.loc[\n",
    "        imputable_entries,\n",
    "    ] = treatments.loc[imputable_entries].mean(axis=1) - controls.loc[imputable_entries].mean(axis=1)\n",
    "    \n",
    "    \n",
    "    # We can get the list of imputed estimates by comparing the null estimates between the imputed and (real) column:\n",
    "    \n",
    "    is_imputed = (~imputed_estimate.isnull()) & (no_estimate)\n",
    "    \n",
    "    # assert that for all non-null estimates imputed estimate is the same (i.e. that we didn't override)\n",
    "    assert_array_equal(imputed_estimate.loc[~no_estimate], estimate.loc[~no_estimate])\n",
    "    \n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'logFC_imputed': imputed_estimate,\n",
    "        'logFC_is_imputed': is_imputed,\n",
    "        'logFC_based_on_single_datapoint': based_on_single_datapoint, \n",
    "    })\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84dc3c9-70b1-4e29-b1b8-cec67cd50925",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_flags_and_imputations = {}\n",
    "\n",
    "\n",
    "results_flags_and_imputations['H3K4me1vsControl'] = flag_and_or_impute(\n",
    "    results['H3K4me1vsControl', 'logFC'],\n",
    "    data_to_model_by_experiment['H3K4me1'],\n",
    "    data_to_model_by_experiment['Controls']\n",
    ")\n",
    "\n",
    "results_flags_and_imputations['H3K4me3vsControl'] = flag_and_or_impute(\n",
    "    results['H3K4me3vsControl', 'logFC'],\n",
    "    data_to_model_by_experiment['H3K4me3'],\n",
    "    data_to_model_by_experiment['Controls']\n",
    ")\n",
    "\n",
    "results_flags_and_imputations['H3K4me3vsH3K4me1'] = flag_and_or_impute(\n",
    "    results['H3K4me3vsH3K4me1', 'logFC'],\n",
    "    data_to_model_by_experiment['H3K4me3'],\n",
    "    data_to_model_by_experiment['H3K4me1']\n",
    ")\n",
    "\n",
    "results_flags_and_imputations = pd.concat(results_flags_and_imputations, axis=1)\n",
    "results_flags_and_imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97924cb6-1699-4a80-a996-73a107e4c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_flags_and_imputations['H3K4me1vsControl', 'logFC_is_imputed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55c50e3-aca6-4fa6-a27b-1581751f9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_flags_and_imputations['H3K4me3vsControl', 'logFC_is_imputed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f2a52-ce29-40d6-9795-a3a1b806af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_flags_and_imputations['H3K4me3vsH3K4me1', 'logFC_is_imputed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6b0aed-9dc3-45a9-98f2-1c368f151bdc",
   "metadata": {},
   "source": [
    "We should augment the data_comment columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff93040-3793-4d1c-92a0-213f2e604412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_string(flags, column):\n",
    "    \n",
    "    if flags['logFC_is_imputed'] and flags['logFC_based_on_single_datapoint']:\n",
    "        return f'logFC({column}) estimation failed and was imputed based on a single datapoint only'\n",
    "    elif flags['logFC_is_imputed']:\n",
    "        return f'logFC({column}) estimation failed and was imputed'\n",
    "    elif flags['logFC_based_on_single_datapoint']:\n",
    "        return f'logFC({column}) estimation was based on a single datapoint only'\n",
    "    elif pd.isnull(flags['logFC_imputed']):\n",
    "        return f'logFC({column}) estimation failed'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b4214-72dc-4f57-a71d-1378bc2008c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_results_flags = {\n",
    "    col: results_flags_and_imputations[col].apply(comment_string, column=col, axis=1) for col in contrasts_matrix_as_df.columns\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e037ba6d-cce0-4a95-8d0c-05f7b05f2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_results_flags['H3K4me3vsH3K4me1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df26a7-6be1-4245-9305-207c5f5910da",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_results_flags['H3K4me1vsControl'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173a5a43-279f-41a4-809f-e553b68bef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_results_flags['H3K4me3vsControl'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b82f8-eda9-4987-b3eb-7457a3531e5c",
   "metadata": {},
   "source": [
    "Let's check the imputation results for the representative null proteins below, a reminder of what the data to model for these proteins looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619cdec9-9f44-43df-ab3a-f365e3e09e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_model.loc[REPRESENTATIVE_NULLS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f32287-fd8a-41e9-b2a1-f6e1525248e9",
   "metadata": {},
   "source": [
    "The corresponding mean estimates for the coefficients minus simple mean of controls (for comparison) where approrpiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6299cb26-dc10-4e74-9a21-d92a4792cd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_model_by_experiment['H3K4me1'].loc[REPRESENTATIVE_NULLS].mean(axis=1) - data_to_model_by_experiment['Controls'].loc[REPRESENTATIVE_NULLS].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947c068f-c3d1-40bc-adef-e8f10f767e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_model_by_experiment['H3K4me3'].loc[REPRESENTATIVE_NULLS].mean(axis=1) - data_to_model_by_experiment['Controls'].loc[REPRESENTATIVE_NULLS].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11d3148-77cc-450b-a673-195479153061",
   "metadata": {},
   "source": [
    "The imputed estimates are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d442d2-c80b-4de1-a156-9d98e6139ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_flags_and_imputations.loc[REPRESENTATIVE_NULLS].loc(axis=1)[:, 'logFC_imputed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a4cde6-67bf-488f-a974-2ca178346f5a",
   "metadata": {},
   "source": [
    "Please verify that they are intuitively correct:\n",
    "\n",
    "1. `PHF8` should have infinite enrichments for `H3K4me1vsControl` and `H3K4me3vsControl`, as it has no estimate for `ExperimentH3` and `ExperimentH4` coefficients (as _all_ values in the two controls are missing), but is detected in `H3K4me3` and `H3K4me1` experiments. \n",
    "2. `FAM98A`, `EIF4G1` and `ZMYND11` should have values imputed with fold change against simple mean of controls as they have estimates for one control (H3 or H4) but not both. \n",
    "3. `H3K4me1vsControl` should be -inf for `PSMC1` and `TUBB2A` , while `H3K4me3vsH3K4me1` should be +inf for these proteins as they have no estimate for `ExperimentH3K4me1`. Similarly impute `SNTB2` to have `H3K4me3vsControl` and `H3K4me3vsH3K4me1`  estimates of negative infinity as this protein has no estimate for `ExperimentH3K4me3`.\n",
    "4. Finally, since `TNN3` has estimates only for the `Control`-associated samples, it should have `H3K4me1vsControl` and `H3K4me3vsControl` imputed to infinity, but no estimate for `H3K4me3vsH3K4me1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00bd3c4-ecf3-4b5c-bc31-42804a83a777",
   "metadata": {},
   "source": [
    "### Heatmaps of imputed proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b304b6c-4f2c-4dd9-9195-368d52e9a4d0",
   "metadata": {},
   "source": [
    "#### H3K4me3vsControl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ad7d7-c582-4383-9ba0-85b31f5c8653",
   "metadata": {},
   "outputs": [],
   "source": [
    "_colname = 'H3K4me3vsControl'\n",
    "_imputed = results_flags_and_imputations.loc[results_flags_and_imputations[_colname, 'logFC_is_imputed'], _colname].sort_values(by='logFC_imputed', ascending=False)\n",
    "_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364b6017-3cf3-40c4-b8f0-c75cf9230e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "figure = plt.figure(figsize=(20*FIVE_MM_IN_INCH, 28*FIVE_MM_IN_INCH), constrained_layout=True)\n",
    "\n",
    "gs = GridSpec(2, 2, figure=figure, width_ratios=(1, 12), height_ratios=(10,1))\n",
    "ax_legends = figure.add_subplot(gs[1,:])\n",
    "ax_legends.axis('off')\n",
    "cax_left = ax_legends.inset_axes([0.02, 0.58, 0.2, 0.2], transform=ax_legends.transAxes)\n",
    "cax_right = ax_legends.inset_axes([0.32, 0.58, 0.2, 0.2], transform=ax_legends.transAxes)\n",
    "\n",
    "cax_left.set_title(\"Imputed logFC estimate\")\n",
    "cax_right.set_title(\"Normalised data\")\n",
    "\n",
    "ax_left = figure.add_subplot(gs[0, 0])\n",
    "ax_right = figure.add_subplot(gs[0, 1], sharey=ax_left)\n",
    "\n",
    "\n",
    "vmax = np.ceil(_imputed['logFC_imputed'].abs().replace(np.inf, np.nan).dropna().max())\n",
    "vmin = -vmax\n",
    "\n",
    "heatmap_right = sns.heatmap(\n",
    "    data_to_model.loc[_imputed.index], \n",
    "    cmap='viridis', robust=True, \n",
    "    ax=ax_right, yticklabels=1, \n",
    "    cbar=True,\n",
    "    cbar_ax = cax_right,\n",
    "    cbar_kws=dict(orientation='horizontal'),\n",
    "    linewidth=0.1,\n",
    "    linecolor='black',\n",
    ")\n",
    "heatmap_left = sns.heatmap(\n",
    "    _imputed[['logFC_imputed']].replace(-np.inf, vmin -1).replace(np.inf, vmax + 1), \n",
    "    cmap='RdBu_r', \n",
    "    center=0, \n",
    "    vmin=vmin, vmax=vmax,\n",
    "    ax=ax_left, yticklabels=1, \n",
    "    cbar=True,\n",
    "    cbar_ax=cax_left,\n",
    "    cbar_kws=dict(orientation='horizontal'),\n",
    "    linewidth=0.1,\n",
    "    linecolor='black',\n",
    ")\n",
    "\n",
    "heatmap_right.xaxis.tick_top()\n",
    "heatmap_right.xaxis.set_tick_params(length=0, rotation=90)\n",
    "\n",
    "heatmap_left.xaxis.tick_top()\n",
    "heatmap_left.xaxis.set_tick_params(length=0, rotation=90)\n",
    "\n",
    "heatmap_right.yaxis.set_tick_params(length=0)\n",
    "heatmap_left.yaxis.set_tick_params(length=0)\n",
    "\n",
    "for ax in [heatmap_left, heatmap_right]:\n",
    "    for spine in ax.spines:\n",
    "        ax.spines[spine].set_visible(True)\n",
    "\n",
    "for tick in heatmap_right.yaxis.get_ticklabels():\n",
    "    tick.set_visible(False)\n",
    "\n",
    "heatmap_right.set_title(f\"Proteins with imputed logFC estimates for {_colname}\")\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / f'06-modelling-imputed-values-for-{_colname}.pdf'\n",
    "_caption = f\"\"\"\n",
    "\n",
    "Heatmap of the imputed `logFC` estimates for the {_colname} coefficient.\n",
    "The proteins are sorted by the estimate, descending.\n",
    "\n",
    "The left heatmap plots the imputed `logFC` estimate, the colour axis limits are set to [{vmin}, {vmax}].\n",
    "Note that some imputed logFC estimates are infinite (which may be both positive or negative). Such points are displayed at the darkest shade of respective colour.\n",
    "\n",
    "The right heatmap shows the data used for modelling in viridis colour scale. \n",
    "Note that all imptued estimates were infered from partial data only.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82fd170-cf23-421c-b094-390a2c871ded",
   "metadata": {},
   "source": [
    "#### H3K4me1vsControl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af7d7c-1f8f-49b3-8761-e64ceb8f272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_colname = 'H3K4me1vsControl'\n",
    "_imputed = results_flags_and_imputations.loc[results_flags_and_imputations[_colname, 'logFC_is_imputed'], _colname].sort_values(by='logFC_imputed', ascending=False)\n",
    "_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2082fab-f551-445d-9e49-50ea58ec5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "figure = plt.figure(figsize=(20*FIVE_MM_IN_INCH, 28*FIVE_MM_IN_INCH), constrained_layout=True)\n",
    "\n",
    "gs = GridSpec(2, 2, figure=figure, width_ratios=(1, 12), height_ratios=(10,1))\n",
    "ax_legends = figure.add_subplot(gs[1,:])\n",
    "ax_legends.axis('off')\n",
    "cax_left = ax_legends.inset_axes([0.02, 0.58, 0.2, 0.2], transform=ax_legends.transAxes)\n",
    "cax_right = ax_legends.inset_axes([0.32, 0.58, 0.2, 0.2], transform=ax_legends.transAxes)\n",
    "\n",
    "cax_left.set_title(\"Imputed logFC estimate\")\n",
    "cax_right.set_title(\"Normalised data\")\n",
    "\n",
    "ax_left = figure.add_subplot(gs[0, 0])\n",
    "ax_right = figure.add_subplot(gs[0, 1], sharey=ax_left)\n",
    "\n",
    "\n",
    "vmax = np.ceil(_imputed['logFC_imputed'].abs().replace(np.inf, np.nan).dropna().max())\n",
    "vmin = -vmax\n",
    "\n",
    "heatmap_right = sns.heatmap(\n",
    "    data_to_model.loc[_imputed.index], \n",
    "    cmap='viridis', robust=True, \n",
    "    ax=ax_right, yticklabels=1, \n",
    "    cbar=True,\n",
    "    cbar_ax = cax_right,\n",
    "    cbar_kws=dict(orientation='horizontal'),\n",
    "    linewidth=0.1,\n",
    "    linecolor='black',\n",
    ")\n",
    "heatmap_left = sns.heatmap(\n",
    "    _imputed[['logFC_imputed']].replace(-np.inf, vmin -1).replace(np.inf, vmax + 1), \n",
    "    cmap='RdBu_r', \n",
    "    center=0, \n",
    "    vmin=vmin, vmax=vmax,\n",
    "    ax=ax_left, yticklabels=1, \n",
    "    cbar=True,\n",
    "    cbar_ax=cax_left,\n",
    "    cbar_kws=dict(orientation='horizontal'),\n",
    "    linewidth=0.1,\n",
    "    linecolor='black',\n",
    ")\n",
    "\n",
    "heatmap_right.xaxis.tick_top()\n",
    "heatmap_right.xaxis.set_tick_params(length=0, rotation=90)\n",
    "\n",
    "heatmap_left.xaxis.tick_top()\n",
    "heatmap_left.xaxis.set_tick_params(length=0, rotation=90)\n",
    "\n",
    "heatmap_right.yaxis.set_tick_params(length=0)\n",
    "heatmap_left.yaxis.set_tick_params(length=0)\n",
    "\n",
    "for ax in [heatmap_left, heatmap_right]:\n",
    "    for spine in ax.spines:\n",
    "        ax.spines[spine].set_visible(True)\n",
    "\n",
    "for tick in heatmap_right.yaxis.get_ticklabels():\n",
    "    tick.set_visible(False)\n",
    "\n",
    "heatmap_right.set_title(f\"Proteins with imputed logFC estimates for {_colname}\")\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / f'06-modelling-imputed-values-for-{_colname}.pdf'\n",
    "_caption = f\"\"\"\n",
    "\n",
    "Heatmap of the imputed `logFC` estimates for the {_colname} coefficient.\n",
    "The proteins are sorted by the estimate, descending.\n",
    "\n",
    "The left heatmap plots the imputed `logFC` estimate, the colour axis limits are set to [{vmin}, {vmax}].\n",
    "Note that some imputed logFC estimates are infinite (which may be both positive or negative). Such points are displayed at the darkest shade of respective colour.\n",
    "\n",
    "The right heatmap shows the data used for modelling in viridis colour scale. \n",
    "Note that all imptued estimates were infered from partial data only.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f3eff5-06ed-4977-8f92-2d18d8cf1fe8",
   "metadata": {},
   "source": [
    "#### H3K4me3vsH3K4me1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb425903-b423-41e1-b6b6-c07936585651",
   "metadata": {},
   "outputs": [],
   "source": [
    "_colname = 'H3K4me3vsH3K4me1'\n",
    "_imputed = results_flags_and_imputations.loc[results_flags_and_imputations[_colname, 'logFC_is_imputed'], _colname].sort_values(by='logFC_imputed', ascending=False)\n",
    "_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b19bc2-9279-4c61-ba2a-4ea4353e84a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "figure = plt.figure(figsize=(20*FIVE_MM_IN_INCH, 28*FIVE_MM_IN_INCH), constrained_layout=True)\n",
    "\n",
    "gs = GridSpec(2, 2, figure=figure, width_ratios=(1, 12), height_ratios=(10,1))\n",
    "ax_legends = figure.add_subplot(gs[1,:])\n",
    "ax_legends.axis('off')\n",
    "cax_left = ax_legends.inset_axes([0.02, 0.58, 0.2, 0.2], transform=ax_legends.transAxes)\n",
    "cax_right = ax_legends.inset_axes([0.32, 0.58, 0.2, 0.2], transform=ax_legends.transAxes)\n",
    "\n",
    "cax_left.set_title(\"Imputed logFC estimate\")\n",
    "cax_right.set_title(\"Normalised data\")\n",
    "\n",
    "ax_left = figure.add_subplot(gs[0, 0])\n",
    "ax_right = figure.add_subplot(gs[0, 1], sharey=ax_left)\n",
    "\n",
    "\n",
    "vmax = np.ceil(_imputed['logFC_imputed'].abs().replace(np.inf, np.nan).dropna().max())\n",
    "if pd.isnull(vmax):\n",
    "    vmax = 1.0\n",
    "\n",
    "vmin = -vmax\n",
    "\n",
    "heatmap_right = sns.heatmap(\n",
    "    data_to_model.loc[_imputed.index], \n",
    "    cmap='viridis', robust=True, \n",
    "    ax=ax_right, yticklabels=1, \n",
    "    cbar=True,\n",
    "    cbar_ax = cax_right,\n",
    "    cbar_kws=dict(orientation='horizontal'),\n",
    "    linewidth=0.1,\n",
    "    linecolor='black',\n",
    ")\n",
    "heatmap_left = sns.heatmap(\n",
    "    _imputed[['logFC_imputed']].replace(-np.inf, vmin -1).replace(np.inf, vmax + 1), \n",
    "    cmap='RdBu_r', \n",
    "    center=0, \n",
    "    vmin=vmin, vmax=vmax,\n",
    "    ax=ax_left, yticklabels=1, \n",
    "    cbar=True,\n",
    "    cbar_ax=cax_left,\n",
    "    cbar_kws=dict(orientation='horizontal'),\n",
    "    linewidth=0.1,\n",
    "    linecolor='black',\n",
    ")\n",
    "\n",
    "heatmap_right.xaxis.tick_top()\n",
    "heatmap_right.xaxis.set_tick_params(length=0, rotation=90)\n",
    "\n",
    "heatmap_left.xaxis.tick_top()\n",
    "heatmap_left.xaxis.set_tick_params(length=0, rotation=90)\n",
    "\n",
    "heatmap_right.yaxis.set_tick_params(length=0)\n",
    "heatmap_left.yaxis.set_tick_params(length=0)\n",
    "\n",
    "for ax in [heatmap_left, heatmap_right]:\n",
    "    for spine in ax.spines:\n",
    "        ax.spines[spine].set_visible(True)\n",
    "\n",
    "for tick in heatmap_right.yaxis.get_ticklabels():\n",
    "    tick.set_visible(False)\n",
    "\n",
    "heatmap_right.set_title(f\"Proteins with imputed logFC estimates for {_colname}\")\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / f'06-modelling-imputed-values-for-{_colname}.pdf'\n",
    "_caption = f\"\"\"\n",
    "\n",
    "Heatmap of the imputed `logFC` estimates for the {_colname} coefficient.\n",
    "The proteins are sorted by the estimate, descending.\n",
    "\n",
    "The left heatmap plots the imputed `logFC` estimate, the colour axis limits are set to [{vmin}, {vmax}].\n",
    "Note that some imputed logFC estimates are infinite (which may be both positive or negative). Such points are displayed at the darkest shade of respective colour.\n",
    "\n",
    "The right heatmap shows the data used for modelling in viridis colour scale. \n",
    "Note that all imptued estimates were infered from partial data only.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91681fb-74d1-43bd-a809-5e649fe0fe36",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "At this point this notebook is finished, so it's time to collect the results and visualise\n",
    "\n",
    "\n",
    "Let's start with a combined \"comment column\",\n",
    "which joins the filtering comment and the flag comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29e76c2-c6a4-49d6-8afb-2a0de7f85a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_comments = pd.DataFrame({'filter_comment': data_comment, 'norm_comment': comments_norm, **comments_results_flags})\n",
    "full_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ff7264-670b-4bd7-8ee9-f89d9e0ce5da",
   "metadata": {},
   "source": [
    "Use a separator of ';'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722ff975-91f5-4582-8380-a3f2f649d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_comments = full_comments.apply(lambda x: '; '.join(x.dropna()), axis=1)\n",
    "full_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b5ddb-7f38-4b96-b3a4-4ce873b27a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_comments.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1023a61-d57c-4ef2-b004-a1666737ed6b",
   "metadata": {},
   "source": [
    "Now gather all the data that we would need to save:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c1bfde-d0fe-4935-9569-10e181984413",
   "metadata": {},
   "outputs": [],
   "source": [
    "_comments = pd.DataFrame(full_comments.copy())\n",
    "_comments.columns = pd.MultiIndex.from_tuples([('comment', 'comment')])\n",
    "\n",
    "_coef_estimates = coef_estimates_design.copy()\n",
    "_coef_estimates.columns = pd.MultiIndex.from_tuples([('coefficient_estimates', c) for c in _coef_estimates.columns])\n",
    "\n",
    "_norm_data = data_numeric_log2_normalised.copy()\n",
    "_norm_data.columns = pd.MultiIndex.from_tuples([('normalised_data', c) for c in _norm_data.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ec8f7a-3778-44db-bb10-225a3053d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results = _norm_data.join(_comments).join(results).join(_coef_estimates).join(results_flags_and_imputations)\n",
    "full_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0195d0-25da-4cde-b161-516903a92aff",
   "metadata": {},
   "source": [
    "For storage, remove multi-index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043454b9-2aa2-47c0-974c-abf553daf977",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results_no_multiindex = full_results.copy()\n",
    "full_results_no_multiindex.columns = ['__'.join(c) for c in full_results.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd9f86d-0c3a-468d-90a0-a8f3b999966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results_no_multiindex.to_csv(OUTPUT_DIRECTORY / '07-output-full_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b2389-4279-457e-80ec-25d294962af3",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228abcb8-93c9-4ba6-bfd9-11f7b1df1b6e",
   "metadata": {},
   "source": [
    "Finally, plot the key results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ac4d2-3b72-4c37-848a-bcca9debf6e3",
   "metadata": {},
   "source": [
    "### Venn diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a3572-cd57-4651-93bc-bab25cd12b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn3_unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1005f2b1-9775-4835-8555-629f6996c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "set_labels = coef_estimates_contrasts.columns\n",
    "set_members = [ set(full_results[full_results[c, 'significant'].fillna(False)].index) for c in set_labels ]\n",
    "\n",
    "venn3_unweighted(set_members, set_labels=set_labels)\n",
    "\n",
    "ax.set_title(\"Venn diagram of proteins of significantly non-zero fold changes\")\n",
    "\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / f'08-venn-diagram-significant.pdf'\n",
    "_caption = f\"\"\"\n",
    "\n",
    "Venn diagram of proteins whose fold change estimates are statistically non zero. (limma, at FDR {FDR_THRESHOLD}, BH correction).\n",
    "Note that the venn diagram areas are not representative of the counts in the slices.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a3627-0189-4c6c-acc6-0a8cb7ed4d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "set_labels = coef_estimates_contrasts.columns\n",
    "\n",
    "\n",
    "set_members = [ \n",
    "     set(full_results[full_results[c, 'significant'].fillna(False)].index) - set(full_results[full_results[c, 'logFC_based_on_single_datapoint'].fillna(False)].index)\n",
    "    for c in coef_estimates_contrasts.columns\n",
    "]\n",
    "venn3_unweighted(set_members, set_labels=set_labels)\n",
    "\n",
    "ax.set_title(\"Venn diagram of proteins of significantly non-zero fold changes\\nthat are based on more than one data point\")\n",
    "\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / f'08-venn-diagram-significant-more-than-one-datapoint.pdf'\n",
    "_caption = f\"\"\"\n",
    "\n",
    "Venn diagram of proteins whose fold change estimates are statistically non zero. (limma, at FDR {FDR_THRESHOLD}, BH correction).\n",
    "Note that the venn diagram areas are not representative of the counts in the slices.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f5dcb-616b-4832-ba92-c04349d90193",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "set_labels = coef_estimates_contrasts.columns\n",
    "\n",
    "\n",
    "set_members = [ \n",
    "     set(full_results[full_results[c, 'significant'].fillna(False)].index) - set(full_results[full_results[c, 'logFC_based_on_single_datapoint'].fillna(False)].index)\n",
    "    for c in coef_estimates_contrasts.columns\n",
    "]\n",
    "venn3_unweighted(set_members, set_labels=set_labels)\n",
    "\n",
    "ax.set_title(\"Venn diagram of proteins of significantly non-zero fold changes\\nthat are based on more than one data point\")\n",
    "\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / f'08-venn-diagram-significant-more-than-one-datapoint.pdf'\n",
    "_caption = f\"\"\"\n",
    "\n",
    "Venn diagram of proteins whose fold change estimates are statistically non zero. (limma, at FDR {FDR_THRESHOLD}, BH correction),\n",
    "and whose estimates are based on >1 data point.\n",
    "Note that the venn diagram areas are not representative of the counts in the slices.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a54f6a3-2933-4970-bc7c-a581cb6beb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "set_labels = coef_estimates_contrasts.columns\n",
    "\n",
    "\n",
    "set_members = [ \n",
    "    set(full_results[full_results[c, 'significant'].fillna(False)].index) \n",
    "        & set(full_results[(full_results['H3K4me1vsControl', 'logFC_imputed'] > 0).fillna(False)].index)\n",
    "        & set(full_results[(full_results['H3K4me3vsControl', 'logFC_imputed'] > 0).fillna(False)].index)\n",
    "    for c in coef_estimates_contrasts.columns\n",
    "]\n",
    "venn3_unweighted(set_members, set_labels=set_labels)\n",
    "\n",
    "ax.set_title(\"Venn diagram of proteins of significantly non-zero fold changes\\ngiven that me1/me3 estimates are positive\")\n",
    "\n",
    "\n",
    "_fname = OUTPUT_DIRECTORY / f'08-venn-diagram-significant-me1me3-positive.pdf'\n",
    "_caption = f\"\"\"\n",
    "\n",
    "Venn diagram of proteins whose fold change estimates are statistically non zero. (limma, at FDR {FDR_THRESHOLD}, BH correction).\n",
    "And whose H3K4me1vsControl and H3K4me3vsControl imputed logFC estimates are greater than zero\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d869da43-4e0d-4caf-bfbf-1fa44b45f28e",
   "metadata": {},
   "source": [
    "### Scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea01362-9d6a-4c4a-b496-92351c126f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from adjustText import adjust_text\n",
    "\n",
    "fig = plt.figure(figsize=(50*FIVE_MM_IN_INCH, 10*FIVE_MM_IN_INCH), constrained_layout=True)\n",
    "\n",
    "axes = fig.subplots(nrows=1, ncols=5, sharex=True, sharey=True)\n",
    "\n",
    "_x = ('H3K4me3vsControl', 'logFC')\n",
    "_y = ('H3K4me1vsControl', 'logFC')\n",
    "_df = full_results.dropna(subset=[_x, _y])\n",
    "_approx_nlabels = 20\n",
    "\n",
    "for ax, col, color in zip(axes, [None] + ['normalisation_proteins'] + list(contrasts_matrix_as_df.columns), sns.color_palette('Set1')):\n",
    "\n",
    "    sns.histplot(\n",
    "        x=_x, \n",
    "        y=_y, \n",
    "        data=_df,\n",
    "        cmap=sns.blend_palette(['#000000','#969696'], as_cmap=True),\n",
    "        ax=ax,\n",
    "    )\n",
    "    \n",
    "    if col is None:\n",
    "        if_ = IsolationForest(random_state=42, contamination=np.clip(_approx_nlabels/len(_df), 1e-5, 0.5))\n",
    "        \n",
    "        outliers = if_.fit_predict(_df[[_x, _y]])\n",
    "        outliers = pd.Series((outliers == -1), index=_df.index)\n",
    "        \n",
    "        texts = []\n",
    "        for ix in outliers[outliers].index:\n",
    "            texts.append(ax.text(_df.loc[ix, _x], _df.loc[ix, _y], ix, color='black'))\n",
    "        \n",
    "        adjust_text(texts, arrowprops=dict(arrowstyle='-'), ax=ax)\n",
    "    elif col == 'normalisation_proteins':\n",
    "        \n",
    "        \n",
    "        ax.scatter(\n",
    "            _df.loc[HISTONES, _x],\n",
    "            _df.loc[HISTONES, _y],\n",
    "            color=color,\n",
    "            edgecolor='black',\n",
    "            alpha=0.8,\n",
    "            s=5,\n",
    "            label=col,\n",
    "        )\n",
    "        \n",
    "        texts = []\n",
    "        for ix in HISTONES:\n",
    "            texts.append(ax.text(_df.loc[ix, _x], _df.loc[ix, _y], ix, color=color))\n",
    "        \n",
    "        adjust_text(texts, arrowprops=dict(arrowstyle='-'), ax=ax)\n",
    "    \n",
    "    else:\n",
    "        subdf = _df[_df[col, 'significant'].fillna(False)]\n",
    "    \n",
    "        if_ = IsolationForest(random_state=42, contamination=np.clip(_approx_nlabels/len(subdf), 1e-5, 0.5))\n",
    "        outliers = if_.fit_predict(subdf[[_x, _y]])\n",
    "        outliers = pd.Series((outliers == -1), index=subdf.index)\n",
    "        \n",
    "        ax.scatter(\n",
    "            subdf[_x],\n",
    "            subdf[_y],\n",
    "            color=color,\n",
    "            edgecolor='black',\n",
    "            alpha=0.8,\n",
    "            s=5,\n",
    "            label=col,\n",
    "        )\n",
    "        \n",
    "        ax.legend(title=\"Significant\")\n",
    "        \n",
    "        texts = []\n",
    "        for ix in outliers[outliers].index:\n",
    "            texts.append(ax.text(_df.loc[ix, _x], _df.loc[ix, _y], ix, color=color))\n",
    "        \n",
    "        adjust_text(texts, arrowprops=dict(arrowstyle='-'), ax=ax)\n",
    "\n",
    "    ax.grid(False)\n",
    "    \n",
    "    ax.axvline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    min_ = np.min([xlim, ylim])\n",
    "    max_ = np.max([xlim, ylim])\n",
    "    \n",
    "    ax.plot([min_, max_], [min_, max_], linestyle=':', color='k')\n",
    "    ax.set_xlim(*xlim, *ylim)\n",
    "    ax.set_xlabel('log2FC(H3K4me3/Controls)')\n",
    "    ax.set_ylabel('log2FC(H3K4me1/Controls)')\n",
    "   \n",
    "    sns.despine(offset=5, ax=ax)\n",
    "    \n",
    "_fname = OUTPUT_DIRECTORY / f'09-scatterplot-full-results_logfc.pdf'\n",
    "_caption = f\"\"\"\n",
    "A scatterplot diagram of model estimates.\n",
    "Log2 fold changes me3 vs controls (H3 and H4) of all proteins are plotted on the x axis, and the log2FC(me1/controls) on the y.\n",
    "\n",
    "The first plot simply plots the whole data, the second plot highlights proteins used to normalise the data, while the remaining three plots highlight proteins for which\n",
    "the corresponding estimates are statistically non zero. (limma, at FDR {FDR_THRESHOLD}, BH correction).\n",
    "Labels are displayed for a handful of outlier proteins.\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941c56f6-9e3d-48e4-b677-e426d2e4a48e",
   "metadata": {},
   "source": [
    "And a zoomed in version of this plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4289a1e-54b5-4a23-a2cc-e6046ed1c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from adjustText import adjust_text\n",
    "\n",
    "fig = plt.figure(figsize=(50*FIVE_MM_IN_INCH, 10*FIVE_MM_IN_INCH), constrained_layout=True)\n",
    "\n",
    "axes = fig.subplots(nrows=1, ncols=5, sharex=True, sharey=True)\n",
    "\n",
    "_x = ('H3K4me3vsControl', 'logFC')\n",
    "_y = ('H3K4me1vsControl', 'logFC')\n",
    "\n",
    "_limits = [-3, 5]\n",
    "\n",
    "_df = full_results.dropna(subset=[_x, _y])\n",
    "_mask = _df[_x].between(*_limits) & _df[_y].between(*_limits)\n",
    "to_drop = _df[~_mask]\n",
    "drop_statement = '{:,} proteins outside of axis limits: {}'.format(\n",
    "    len(to_drop), ['{} (x={:.2f}, y={:.2f})'.format(ix, row[_x], row[_y]) for ix, row in to_drop.sort_index().iterrows()]\n",
    ")\n",
    "\n",
    "_df = _df[_mask]\n",
    "\n",
    "_approx_nlabels = 20\n",
    "\n",
    "for ax, col, color in zip(axes, [None] + ['normalisation_proteins'] + list(contrasts_matrix_as_df.columns), sns.color_palette('Set1')):\n",
    "\n",
    "    sns.histplot(\n",
    "        x=_x, \n",
    "        y=_y, \n",
    "        data=_df,\n",
    "        cmap=sns.blend_palette(['#000000','#969696'], as_cmap=True),\n",
    "        ax=ax,\n",
    "    )\n",
    "    \n",
    "    if col is None:\n",
    "        if_ = IsolationForest(random_state=42, contamination=np.clip(_approx_nlabels/len(_df), 1e-5, 0.5))\n",
    "        \n",
    "        outliers = if_.fit_predict(_df[[_x, _y]])\n",
    "        outliers = pd.Series((outliers == -1), index=_df.index)\n",
    "        \n",
    "        texts = []\n",
    "        for ix in outliers[outliers].index:\n",
    "            texts.append(ax.text(_df.loc[ix, _x], _df.loc[ix, _y], ix, color='black'))\n",
    "        \n",
    "        adjust_text(texts, arrowprops=dict(arrowstyle='-'), ax=ax)\n",
    "    elif col == 'normalisation_proteins':\n",
    "        \n",
    "        \n",
    "        ax.scatter(\n",
    "            _df.loc[HISTONES, _x],\n",
    "            _df.loc[HISTONES, _y],\n",
    "            color=color,\n",
    "            edgecolor='black',\n",
    "            alpha=0.8,\n",
    "            s=5,\n",
    "            label=col,\n",
    "        )\n",
    "        \n",
    "        texts = []\n",
    "        for ix in HISTONES:\n",
    "            texts.append(ax.text(_df.loc[ix, _x], _df.loc[ix, _y], ix, color=color))\n",
    "        \n",
    "        adjust_text(texts, arrowprops=dict(arrowstyle='-'), ax=ax)\n",
    "    \n",
    "    else:\n",
    "        subdf = _df[_df[col, 'significant'].fillna(False)]\n",
    "    \n",
    "        if_ = IsolationForest(random_state=42, contamination=np.clip(_approx_nlabels/len(subdf), 1e-5, 0.5))\n",
    "        outliers = if_.fit_predict(subdf[[_x, _y]])\n",
    "        outliers = pd.Series((outliers == -1), index=subdf.index)\n",
    "        \n",
    "        ax.scatter(\n",
    "            subdf[_x],\n",
    "            subdf[_y],\n",
    "            color=color,\n",
    "            edgecolor='black',\n",
    "            alpha=0.8,\n",
    "            s=5,\n",
    "            label=col,\n",
    "        )\n",
    "        \n",
    "        ax.legend(title=\"Significant\")\n",
    "        ax.set_xlim(*_limits)\n",
    "        ax.set_ylim(*_limits)\n",
    "        texts = []\n",
    "        for ix in outliers[outliers].index:\n",
    "            texts.append(ax.text(_df.loc[ix, _x], _df.loc[ix, _y], ix, color=color))\n",
    "        \n",
    "        adjust_text(texts, arrowprops=dict(arrowstyle='-'), ax=ax)\n",
    "\n",
    "    ax.grid(False)\n",
    "    \n",
    "    ax.axvline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    min_ = np.min([xlim, ylim])\n",
    "    max_ = np.max([xlim, ylim])\n",
    "    \n",
    "    ax.plot([min_, max_], [min_, max_], linestyle=':', color='k')\n",
    "    ax.set_xlim(*xlim, *ylim)\n",
    "    ax.set_xlabel('log2FC(H3K4me3/Controls)')\n",
    "    ax.set_ylabel('log2FC(H3K4me1/Controls)')\n",
    "   \n",
    "    sns.despine(offset=5, ax=ax)\n",
    "    \n",
    "_fname = OUTPUT_DIRECTORY / f'09-scatterplot-full-results_logfc_zoomed_in.pdf'\n",
    "_caption = f\"\"\"\n",
    "A scatterplot diagram of model estimates.\n",
    "Log2 fold changes me3 vs controls (H3 and H4) of all proteins are plotted on the x axis, and the log2FC(me1/controls) on the y.\n",
    "\n",
    "The first plot simply plots the whole data, the second plot highlights proteins used to normalise the data, while the remaining three plots highlight proteins for which\n",
    "the corresponding estimates are statistically non zero. (limma, at FDR {FDR_THRESHOLD}, BH correction).\n",
    "Labels are displayed for a handful of outlier proteins.\n",
    "\n",
    "Note that this plot zooms in to the area defined by the following limits: {_limits}.\n",
    "{drop_statement}\n",
    "\"\"\"\n",
    "plt.savefig(_fname, bbox_inches='tight', dpi=DPI)\n",
    "with open(str(_fname) + '.caption.md', 'w') as f:\n",
    "    f.write(_caption)\n",
    "    print(_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7663160-5857-4c2a-b80a-bf15d608308a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04979971-ed6b-40e6-8b71-b66bf74dd043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
